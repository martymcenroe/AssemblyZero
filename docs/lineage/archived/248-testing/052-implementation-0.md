# File: tests/test_issue_248.py

```python
"""Test file for Issue #248.

Generated by AgentOS TDD Testing Workflow.
Tests for Gemini answering open questions before human escalation.
"""

import pytest
import re
from pathlib import Path
from unittest.mock import Mock, patch


# Unit Tests
# -----------

def test_id():
    """
    Test Description | Expected Behavior | Status
    Basic sanity test to verify test file is working.
    """
    # This is a placeholder test - it verifies the test infrastructure works
    # The real tests are below with specific functionality
    assert True, "Test infrastructure working"


def test_t010():
    """
    test_draft_with_questions_proceeds_to_review | Draft not blocked
    pre-review | RED
    """
    # TDD: Arrange
    from agentos.workflows.requirements.graph import route_after_generate_draft

    # State with a draft that has open questions - should NOT be blocked
    state = {
        "error_message": "",
        "config_gates_draft": False,  # Skip human gate
        # Note: no pre-review validation check happens anymore
    }

    # TDD: Act
    result = route_after_generate_draft(state)

    # TDD: Assert - should go to review, not END or blocked
    assert result == "N3_review", "Draft with open questions should proceed to review"


def test_t020():
    """
    test_gemini_answers_questions | Questions resolved in verdict | RED
    """
    # TDD: Arrange
    from agentos.workflows.requirements.nodes.review import _check_open_questions_status

    draft_with_questions = """# LLD

### Open Questions
- [ ] Should we use caching?
- [ ] What database backend?
"""

    verdict_with_resolutions = """## Open Questions Resolved
- [x] ~~Should we use caching?~~ **RESOLVED: Yes, use Redis for session caching.**
- [x] ~~What database backend?~~ **RESOLVED: PostgreSQL for reliability.**

## Verdict
[x] **APPROVED**
"""

    # TDD: Act
    status = _check_open_questions_status(draft_with_questions, verdict_with_resolutions)

    # TDD: Assert
    assert status == "RESOLVED", f"Questions should be RESOLVED, got {status}"


def test_t030():
    """
    test_unanswered_triggers_loop | Loop back to N3 with followup | RED
    """
    # TDD: Arrange
    from agentos.workflows.requirements.graph import route_after_review

    # State where questions exist but weren't answered
    state = {
        "error_message": "",
        "config_gates_verdict": False,
        "lld_status": "APPROVED",  # Even if approved, unanswered questions trigger loop
        "open_questions_status": "UNANSWERED",
        "iteration_count": 3,
        "max_iterations": 20,
    }

    # TDD: Act
    result = route_after_review(state)

    # TDD: Assert - should loop back to drafter (N1) for revision
    assert result == "N1_generate_draft", f"Unanswered questions should loop to N1, got {result}"


def test_t040():
    """
    test_human_required_escalates | Goes to human gate | RED
    """
    # TDD: Arrange
    from agentos.workflows.requirements.graph import route_after_review

    # State where Gemini marked questions as HUMAN REQUIRED
    state = {
        "error_message": "",
        "config_gates_verdict": False,  # Even with gates disabled
        "lld_status": "APPROVED",
        "open_questions_status": "HUMAN_REQUIRED",
    }

    # TDD: Act
    result = route_after_review(state)

    # TDD: Assert - MUST go to human gate N4
    assert result == "N4_human_gate_verdict", f"HUMAN_REQUIRED should go to N4, got {result}"


def test_t050():
    """
    test_max_iterations_respected | Terminates after limit | RED
    """
    # TDD: Arrange
    from agentos.workflows.requirements.graph import route_after_review

    # State at max iterations with unanswered questions
    state = {
        "error_message": "",
        "config_gates_verdict": False,
        "lld_status": "APPROVED",
        "open_questions_status": "UNANSWERED",
        "iteration_count": 20,
        "max_iterations": 20,
    }

    # TDD: Act
    result = route_after_review(state)

    # TDD: Assert - at max iterations, go to human gate instead of looping
    assert result == "N4_human_gate_verdict", f"Max iterations should go to human gate, got {result}"


def test_t060():
    """
    test_all_answered_proceeds_to_finalize | N5 reached when resolved |
    RED
    """
    # TDD: Arrange
    from agentos.workflows.requirements.graph import route_after_review

    # State where all questions are resolved and LLD approved
    state = {
        "error_message": "",
        "config_gates_verdict": False,
        "lld_status": "APPROVED",
        "open_questions_status": "RESOLVED",
    }

    # TDD: Act
    result = route_after_review(state)

    # TDD: Assert - should proceed to finalize
    assert result == "N5_finalize", f"Resolved questions with APPROVED should go to N5, got {result}"


def test_t070():
    """
    test_prompt_includes_question_instructions | 0702c has new section |
    RED
    """
    # TDD: Arrange
    prompt_paths = [
        Path("docs/skills/0702c-LLD-Review-Prompt.md"),
        Path("C:/Users/mcwiz/Projects/AgentOS-248/docs/skills/0702c-LLD-Review-Prompt.md"),
    ]

    content = None
    for path in prompt_paths:
        if path.exists():
            content = path.read_text()
            break

    # TDD: Act & Assert
    if content is None:
        pytest.skip("Prompt file not found in expected locations")

    # Check for Open Questions Protocol section
    assert "Open Questions Protocol" in content, "Should have Open Questions Protocol section"
    assert "RESOLVED:" in content, "Should have RESOLVED: format instruction"


def test_010():
    """
    Draft with open questions proceeds | Auto | Draft with 3 unchecked
    questions | Reaches N3_review | No BLOCKED status pre-review
    """
    # TDD: Arrange
    from agentos.workflows.requirements.graph import route_after_generate_draft

    # Simulate state after draft generation (no blocking happens anymore)
    state = {
        "error_message": "",
        "config_gates_draft": False,
    }

    # TDD: Act
    result = route_after_generate_draft(state)

    # TDD: Assert
    assert result == "N3_review", "Draft should proceed to review regardless of open questions"


def test_020():
    """
    Gemini answers questions | Auto | Review with question instructions |
    All questions [x] | Verdict contains resolutions
    """
    # TDD: Arrange
    from agentos.workflows.requirements.nodes.review import _verdict_has_resolved_questions

    verdict_with_resolved = """## Open Questions Resolved
- [x] ~~Question 1~~ **RESOLVED: Use approach A.**
- [x] ~~Question 2~~ **RESOLVED: Use approach B.**
"""

    # TDD: Act
    has_resolutions = _verdict_has_resolved_questions(verdict_with_resolved)

    # TDD: Assert
    assert has_resolutions is True, "Should detect resolved questions in verdict"


def test_030():
    """
    Unanswered triggers loop | Auto | Verdict approves but questions
    unchecked | Loop to N3 | Followup prompt sent
    """
    # TDD: Arrange
    from agentos.workflows.requirements.nodes.review import _check_open_questions_status

    draft_with_questions = """### Open Questions
- [ ] Unanswered question"""

    verdict_without_answers = "[x] **APPROVED** - great design"

    # TDD: Act
    status = _check_open_questions_status(draft_with_questions, verdict_without_answers)

    # TDD: Assert
    assert status == "UNANSWERED", f"Should be UNANSWERED when verdict doesn't address questions, got {status}"


def test_040():
    """
    HUMAN REQUIRED escalates | Auto | Verdict with HUMAN REQUIRED | Goes
    to N4 | Human gate invoked
    """
    # TDD: Arrange
    from agentos.workflows.requirements.nodes.review import _verdict_has_human_required

    verdict_variants = [
        "This question HUMAN REQUIRED for approval.",
        "This requires **HUMAN REQUIRED** decision.",
        "NEEDS HUMAN DECISION on architecture.",
        "ESCALATE TO HUMAN for this choice.",
    ]

    # TDD: Act & Assert
    for verdict in verdict_variants:
        assert _verdict_has_human_required(verdict) is True, f"Should detect HUMAN REQUIRED in: {verdict}"


def test_050():
    """
    Max iterations respected | Auto | 20 loops without resolution |
    Terminates | Exit with current state
    """
    # TDD: Arrange
    from agentos.workflows.requirements.graph import route_after_review

    # At exactly max iterations
    state = {
        "error_message": "",
        "config_gates_verdict": False,
        "open_questions_status": "UNANSWERED",
        "iteration_count": 20,
        "max_iterations": 20,
    }

    # TDD: Act
    result = route_after_review(state)

    # TDD: Assert - should go to human gate, not loop forever
    assert result == "N4_human_gate_verdict", "Should go to human gate at max iterations"


def test_060():
    """
    Resolved proceeds to finalize | Auto | All questions answered |
    Reaches N5 | APPROVED status
    """
    # TDD: Arrange
    from agentos.workflows.requirements.graph import route_after_review

    state = {
        "error_message": "",
        "config_gates_verdict": False,
        "lld_status": "APPROVED",
        "open_questions_status": "RESOLVED",
    }

    # TDD: Act
    result = route_after_review(state)

    # TDD: Assert
    assert result == "N5_finalize", "Resolved + APPROVED should finalize"


def test_070():
    """
    Prompt updated | Auto | Load 0702c | Contains question instructions |
    Regex match
    """
    # TDD: Arrange
    prompt_paths = [
        Path("docs/skills/0702c-LLD-Review-Prompt.md"),
        Path("C:/Users/mcwiz/Projects/AgentOS-248/docs/skills/0702c-LLD-Review-Prompt.md"),
    ]

    content = None
    for path in prompt_paths:
        if path.exists():
            content = path.read_text()
            break

    # TDD: Act & Assert
    if content is None:
        pytest.skip("Prompt file not found")

    # Check for specific format instruction pattern
    assert re.search(r"\[x\].*~~.*~~.*RESOLVED:", content, re.IGNORECASE), \
        "Should have format: [x] ~~question~~ **RESOLVED:**"
```