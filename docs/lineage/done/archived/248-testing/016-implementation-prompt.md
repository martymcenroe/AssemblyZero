# Implementation Request

## Context

You are implementing code for Issue #248 using TDD.
This is iteration 1 of the implementation.

## Requirements

The tests have been scaffolded and need implementation code to pass.

### LLD Summary

# 248 - Feature: Gemini Answers Open Questions Before Human Escalation

## 1. Context & Goal
* **Issue:** #248
* **Objective:** Move validation gate AFTER Gemini review so Gemini can answer open questions, with loop-back until resolved
* **Status:** Approved (Gemini 3 Pro, 2026-02-03)
* **Related Issues:** #245 (validation counted DoD checkboxes), #236 (original validation gate)

### Open Questions
*Questions that need clarification before or during implementation. Remove when resolved.*

- [x] ~~Should we add a max retry count for the Gemini question-answering loop, or reuse the existing max_iterations budget?~~ **RESOLVED: Reuse the existing `max_iterations` budget.** Adding a separate counter introduces unnecessary state complexity. The global budget suffices to prevent infinite loops.
- [x] ~~What should happen if Gemini marks ALL questions as HUMAN REQUIRED - terminate workflow or force human gate?~~ **RESOLVED: Force Human Gate (N4).** Terminating the workflow defeats the purpose of "Human Required". The workflow must escalate to the human for resolution.
- [x] ~~Should the prompt changes be in the template file (0702c) or hardcoded in review.py?~~ **RESOLVED: Template file (0702c).** Prompts must be treated as versioned assets/data, not hardcoded strings in logic files.

## 2. Proposed Changes

*This section is the **source of truth** for implementation. Describe exactly what will be built.*

### 2.1 Files Changed

| File | Change Type | Description |
|------|-------------|-------------|
| `agentos/workflows/requirements/nodes/generate_draft.py` | Modify | Remove pre-review validation gate (lines ~245-280) |
| `agentos/workflows/requirements/nodes/review.py` | Modify | Add post-review open questions check |
| `agentos/workflows/requirements/graph.py` | Modify | Add conditional edge for question-loop after review |
| `docs/skills/0702c-LLD-Review-Prompt.md` | Modify | Add Open Questions answering instructions |
| `tests/unit/test_open_questions_loop.py` | Add ...

### Test Scenarios

- **test_id**: Test Description | Expected Behavior | Status
  - Requirement: 
  - Type: unit

- **test_t010**: test_draft_with_questions_proceeds_to_review | Draft not blocked pre-review | RED
  - Requirement: 
  - Type: unit

- **test_t020**: test_gemini_answers_questions | Questions resolved in verdict | RED
  - Requirement: 
  - Type: unit

- **test_t030**: test_unanswered_triggers_loop | Loop back to N3 with followup | RED
  - Requirement: 
  - Type: unit

- **test_t040**: test_human_required_escalates | Goes to human gate | RED
  - Requirement: 
  - Type: unit

- **test_t050**: test_max_iterations_respected | Terminates after limit | RED
  - Requirement: 
  - Type: unit

- **test_t060**: test_all_answered_proceeds_to_finalize | N5 reached when resolved | RED
  - Requirement: 
  - Type: unit

- **test_t070**: test_prompt_includes_question_instructions | 0702c has new section | RED
  - Requirement: 
  - Type: unit

- **test_010**: Draft with open questions proceeds | Auto | Draft with 3 unchecked questions | Reaches N3_review | No BLOCKED status pre-review
  - Requirement: 
  - Type: unit

- **test_020**: Gemini answers questions | Auto | Review with question instructions | All questions [x] | Verdict contains resolutions
  - Requirement: 
  - Type: unit

- **test_030**: Unanswered triggers loop | Auto | Verdict approves but questions unchecked | Loop to N3 | Followup prompt sent
  - Requirement: 
  - Type: unit

- **test_040**: HUMAN REQUIRED escalates | Auto | Verdict with HUMAN REQUIRED | Goes to N4 | Human gate invoked
  - Requirement: 
  - Type: unit

- **test_050**: Max iterations respected | Auto | 20 loops without resolution | Terminates | Exit with current state
  - Requirement: 
  - Type: unit

- **test_060**: Resolved proceeds to finalize | Auto | All questions answered | Reaches N5 | APPROVED status
  - Requirement: 
  - Type: unit

- **test_070**: Prompt updated | Auto | Load 0702c | Contains question instructions | Regex match
  - Requirement: 
  - Type: unit

### Test File: C:\Users\mcwiz\Projects\AgentOS-248\tests\test_issue_248.py

```python
"""Test file for Issue #248.

Generated by AgentOS TDD Testing Workflow.
Each test starts with `assert False` - implementation will make them pass.
"""

import pytest


# Unit Tests
# -----------

def test_id():
    """
    Test Description | Expected Behavior | Status
    """
    # TDD: Arrange
    # TODO: Set up test data and mocks

    # TDD: Act
    # TODO: Call the function/method under test

    # TDD: Assert
    assert False, "TDD: Implementation pending for test_id"


def test_t010():
    """
    test_draft_with_questions_proceeds_to_review | Draft not blocked
    pre-review | RED
    """
    # TDD: Arrange
    # TODO: Set up test data and mocks

    # TDD: Act
    # TODO: Call the function/method under test

    # TDD: Assert
    assert False, "TDD: Implementation pending for test_t010"


def test_t020():
    """
    test_gemini_answers_questions | Questions resolved in verdict | RED
    """
    # TDD: Arrange
    # TODO: Set up test data and mocks

    # TDD: Act
    # TODO: Call the function/method under test

    # TDD: Assert
    assert False, "TDD: Implementation pending for test_t020"


def test_t030():
    """
    test_unanswered_triggers_loop | Loop back to N3 with followup | RED
    """
    # TDD: Arrange
    # TODO: Set up test data and mocks

    # TDD: Act
    # TODO: Call the function/method under test

    # TDD: Assert
    assert False, "TDD: Implementation pending for test_t030"


def test_t040():
    """
    test_human_required_escalates | Goes to human gate | RED
    """
    # TDD: Arrange
    # TODO: Set up test data and mocks

    # TDD: Act
    # TODO: Call the function/method under test

    # TDD: Assert
    assert False, "TDD: Implementation pending for test_t040"


def test_t050():
    """
    test_max_iterations_respected | Terminates after limit | RED
    """
    # TDD: Arrange
    # TODO: Set up test data and mocks

    # TDD: Act
    # TODO: Call the function/method under test

    # TDD: Assert
    assert False, "TDD: Implementation pending for test_t050"


def test_t060():
    """
    test_all_answered_proceeds_to_finalize | N5 reached when resolved |
    RED
    """
    # TDD: Arrange
    # TODO: Set up test data and mocks

    # TDD: Act
    # TODO: Call the function/method under test

    # TDD: Assert
    assert False, "TDD: Implementation pending for test_t060"


def test_t070():
    """
    test_prompt_includes_question_instructions | 0702c has new section |
    RED
    """
    # TDD: Arrange
    # TODO: Set up test data and mocks

    # TDD: Act
    # TODO: Call the function/method under test

    # TDD: Assert
    assert False, "TDD: Implementation pending for test_t070"


def test_010():
    """
    Draft with open questions proceeds | Auto | Draft with 3 unchecked
    questions | Reaches N3_review | No BLOCKED status pre-review
    """
    # TDD: Arrange
    # TODO: Set up test data and mocks

    # TDD: Act
    # TODO: Call the function/method under test

    # TDD: Assert
    assert False, "TDD: Implementation pending for test_010"


def test_020():
    """
    Gemini answers questions | Auto | Review with question instructions |
    All questions [x] | Verdict contains resolutions
    """
    # TDD: Arrange
    # TODO: Set up test data and mocks

    # TDD: Act
    # TODO: Call the function/method under test

    # TDD: Assert
    assert False, "TDD: Implementation pending for test_020"


def test_030():
    """
    Unanswered triggers loop | Auto | Verdict approves but questions
    unchecked | Loop to N3 | Followup prompt sent
    """
    # TDD: Arrange
    # TODO: Set up test data and mocks

    # TDD: Act
    # TODO: Call the function/method under test

    # TDD: Assert
    assert False, "TDD: Implementation pending for test_030"


def test_040():
    """
    HUMAN REQUIRED escalates | Auto | Verdict with HUMAN REQUIRED | Goes
    to N4 | Human gate invoked
    """
    # TDD: Arrange
    # TODO: Set up test data and mocks

    # TDD: Act
    # TODO: Call the function/method under test

    # TDD: Assert
    assert False, "TDD: Implementation pending for test_040"


def test_050():
    """
    Max iterations respected | Auto | 20 loops without resolution |
    Terminates | Exit with current state
    """
    # TDD: Arrange
    # TODO: Set up test data and mocks

    # TDD: Act
    # TODO: Call the function/method under test

    # TDD: Assert
    assert False, "TDD: Implementation pending for test_050"


def test_060():
    """
    Resolved proceeds to finalize | Auto | All questions answered |
    Reaches N5 | APPROVED status
    """
    # TDD: Arrange
    # TODO: Set up test data and mocks

    # TDD: Act
    # TODO: Call the function/method under test

    # TDD: Assert
    assert False, "TDD: Implementation pending for test_060"


def test_070():
    """
    Prompt updated | Auto | Load 0702c | Contains question instructions |
    Regex match
    """
    # TDD: Arrange
    # TODO: Set up test data and mocks

    # TDD: Act
    # TODO: Call the function/method under test

    # TDD: Assert
    assert False, "TDD: Implementation pending for test_070"


```

### Source Files to Modify

These are the existing files you need to modify:

#### agentos/workflows/requirements/nodes/generate_draft.py (Modify)

Remove pre-review validation gate (lines ~245-280)

```python
"""N1: Generate draft node for Requirements Workflow.

Issue #101: Unified Requirements Workflow
Issue #248: Remove pre-review validation gate - Gemini answers open questions

Uses the configured drafter LLM to generate a draft based on:
- Issue workflow: brief content + template
- LLD workflow: issue content + context + template

Supports revision mode with cumulative verdict history.
"""

import re
from pathlib import Path
from typing import Any

from agentos.core.llm_provider import get_provider
from agentos.workflows.requirements.audit import (
    load_template,
    next_file_number,
    save_audit_file,
)
from agentos.workflows.requirements.state import RequirementsWorkflowState


def generate_draft(state: RequirementsWorkflowState) -> dict[str, Any]:
    """N1: Generate draft using configured drafter.

    Steps:
    1. Load template from agentos_root
    2. Build prompt (initial or revision)
    3. Call drafter LLM
    4. Save draft to audit trail
    5. Increment draft_count

    Note (Issue #248): Pre-review validation gate removed.
    Open questions now proceed to review where Gemini can answer them.

    Args:
        state: Current workflow state.

    Returns:
        State updates with current_draft, draft_count.
    """
    workflow_type = state.get("workflow_type", "lld")
    agentos_root = Path(state.get("agentos_root", ""))
    target_repo = Path(state.get("target_repo", ""))
    mock_mode = state.get("config_mock_mode", False)
    audit_dir = Path(state.get("audit_dir", ""))

    draft_count = state.get("draft_count", 0) + 1
    is_revision = bool(state.get("current_draft") and state.get("verdict_history"))

    if is_revision:
        print(f"\n[N1] Generating revision (draft #{draft_count})...")
    else:
        print(f"\n[N1] Generating initial draft...")

    # Use mock provider in mock mode, otherwise use configured drafter
    if mock_mode:
        drafter_spec = "mock:draft"
    else:
        drafter_spec = state.get("config_drafter", "claude:opus-4.5")

    # Determine template path based on workflow type
    if workflow_type == "issue":
        template_path = Path("docs/templates/0101-issue-template.md")
    else:
        template_path = Path("docs/templates/0102-feature-lld-template.md")

    # Load template
    try:
        template = load_template(template_path, agentos_root)
    except FileNotFoundError as e:
        return {"error_message": str(e)}

    # Build prompt
    prompt = _build_prompt(state, template, workflow_type)

    # Get drafter provider
    try:
        drafter = get_provider(drafter_spec)
    except ValueError as e:
        return {"error_message": f"Invalid drafter: {e}"}

    # System prompt for drafting
    if workflow_type == "issue":
        system_prompt = """You are a technical writer creating a GitHub issue.

CRITICAL FORMATTING RULES:
- Start DIRECTLY with the issue title (# heading)
- Do NOT include any preamble, explanation, or meta-commentary
- Output ONLY the raw markdown content that will be pasted into GitHub
- First line MUST be the issue title starting with #

Use the template structure provided. Fill in all sections. Be specific and actionable."""
    else:
        system_prompt = """You are a technical architect creating a Low-Level Design document.

CRITICAL FORMATTING RULES:
- Start DIRECTLY with the document title (# heading)
- Do NOT include any preamble, explanation, or meta-commentary
- Output ONLY the raw markdown content
- First line MUST be the title starting with #

Use the template structure provided. Include all sections. Be specific about:
- Files to be created/modified
- Function signatures
- Data structures
- Error handling approach"""

    # Call drafter
    print(f"    Drafter: {drafter_spec}")

    result = drafter.invoke(system_prompt=system_prompt, content=prompt)

    if not result.success:
        print(f"    ERROR: {result.error_message}")
        return {"error_message": f"Drafter failed: {result.error_message}"}

    draft_content = result.response or ""

    # Save to audit trail
    iteration_count = state.get("iteration_count", 0) + 1
    file_num = next_file_number(audit_dir)
    if audit_dir.exists():
        draft_path = save_audit_file(audit_dir, file_num, "draft.md", draft_content)
    else:
        draft_path = None

    draft_lines = len(draft_content.splitlines()) if draft_content else 0
    print(f"    Generated {draft_lines} lines")
    if draft_path:
        print(f"    Saved: {draft_path.name}")

    # Issue #248: Pre-review validation gate REMOVED
    # Open questions now proceed to review where Gemini can answer them.
    # The post-review check in review.py handles the loop-back logic.

    return {
        "current_draft": draft_content,
        "current_draft_path": str(draft_path) if draft_path else "",
        "draft_count": draft_count,
        "iteration_count": iteration_count,
        "file_counter": file_num,
        "user_feedback": "",  # Clear feedback after use
        "error_message": "",
    }


def _build_prompt(
    state: RequirementsWorkflowState,
    template: str,
    workflow_type: str,
) -> str:
    """Build prompt for drafter based on workflow type and revision state.

    Args:
        state: Current workflow state.
        template: Template content.
        workflow_type: Either "issue" or "lld".

    Returns:
        Complete prompt string.
    """
    current_draft = state.get("current_draft", "")
    verdict_history = state.get("verdict_history", [])
    user_feedback = state.get("user_feedback", "")

    if workflow_type == "issue":
        input_content = state.get("brief_content", "")
        input_label = "Brief (user's ideation notes)"
    else:
        issue_number = state.get("issue_number", 0)
        issue_title = state.get("issue_title", "")
        issue_body = state.get("issue_body", "")
        context_content = state.get("context_content", "")

        # CRITICAL: Explicitly include issue number to prevent LLM confusion
        input_content = f"# Issue #{issue_number}: {issue_title}\n\n{issue_body}"
        if context_content:
            input_content += f"\n\n## Context\n\n{context_content}"
        input_content += f"\n\n**CRITICAL: This LLD is for GitHub Issue #{issue_number}. Use this exact issue number in all references.**"
        input_label = f"GitHub Issue #{issue_number}"

    # Check if this is a revision
    if current_draft and verdict_history:
        # Revision mode
        revision_context = ""

        if verdict_history:
            revision_context += "## ALL Gemini Review Feedback (CUMULATIVE)\n\n"
            for i, verdict in enumerate(verdict_history, 1):
                revision_context += f"### Gemini Review #{i}\n\n{verdict}\n\n"

        if user_feedback:
            revision_context += f"## Additional Human Feedback\n\n{user_feedback}\n\n"

        prompt = f"""IMPORTANT: Output ONLY the markdown content. Start with # title. No preamble.

{revision_context}## Current Draft (to revise)
{current_draft}

## Original {input_label}
{input_content}

## Template (REQUIRED STRUCTURE)
{template}

CRITICAL REVISION INSTRUCTIONS:
1. Implement EVERY change requested by Gemini feedback
2. PRESERVE sections that Gemini didn't flag
3. ONLY modify sections Gemini specifically mentioned
4. Keep ALL template sections intact

Revise the draft to address ALL feedback above.
START YOUR RESPONSE WITH THE # HEADING. NO PREAMBLE."""

    else:
        # Initial draft mode
        prompt = f"""IMPORTANT: Output ONLY the markdown content. Start with # title. No preamble.

## {input_label}
{input_content}

## Template (follow this structure)
{template}

Create a complete document following the template structure.
START YOUR RESPONSE WITH THE # HEADING. NO PREAMBLE."""

    return prompt


def validate_draft_structure(content: str) -> str | None:
    """Check for unresolved open questions in draft.

    Issue #235: Mechanical validation gate to catch structural issues
    before Gemini review.

    Issue #245: Only checks the 'Open Questions' section, ignoring Definition
    of Done and other sections that legitimately have unchecked checkboxes.

    Issue #248: This function is kept for backward compatibility but is NO LONGER
    called in the main generate_draft flow. Open questions now proceed to review
    where Gemini can answer them.

    Args:
        content: Draft content to validate.

    Returns:
        Error message if validation fails, None if passes.
    """
    if not content:
        return None

    # Extract only the Open Questions section
    # Pattern: from "### Open Questions" or "## Open Questions"
    # until the next "##" header or end of document
    pattern = r"(?:^##?#?\s*Open Questions\s*\n)(.*?)(?=^##|\Z)"
    match = re.search(pattern, content, re.MULTILINE | re.DOTALL)

    if not match:
        # No Open Questions section found - that's fine
        return None

    open_questions_section = match.group(1)

    # Count unchecked boxes only in this section
    unchecked = re.findall(r"^- \[ \]", open_questions_section, re.MULTILINE)
    if unchecked:
        return f"BLOCKED: {len(unchecked)} unresolved open questions - resolve before review"

    return None
```

#### agentos/workflows/requirements/nodes/review.py (Modify)

Add post-review open questions check

```python
"""N3: Review node for Requirements Workflow.

Issue #101: Unified Requirements Workflow
Issue #248: Add post-review open questions check

Uses the configured reviewer LLM to review the current draft.
Saves verdict to audit trail and updates verdict history.
"""

import re
from pathlib import Path
from typing import Any

from agentos.core.llm_provider import get_provider
from agentos.workflows.requirements.audit import (
    load_review_prompt,
    next_file_number,
    save_audit_file,
)
from agentos.workflows.requirements.state import RequirementsWorkflowState


def review(state: RequirementsWorkflowState) -> dict[str, Any]:
    """N3: Review draft using configured reviewer.

    Steps:
    1. Load review prompt from agentos_root
    2. Build review content (draft + context)
    3. Call reviewer LLM
    4. Save verdict to audit trail
    5. Update verdict_count and verdict_history
    6. Determine lld_status from verdict
    7. Check for open questions resolution (Issue #248)

    Args:
        state: Current workflow state.

    Returns:
        State updates with current_verdict, verdict_count, verdict_history,
        and open_questions_status.
    """
    workflow_type = state.get("workflow_type", "lld")
    agentos_root = Path(state.get("agentos_root", ""))
    mock_mode = state.get("config_mock_mode", False)
    audit_dir = Path(state.get("audit_dir", ""))
    current_draft = state.get("current_draft", "")
    verdict_history = list(state.get("verdict_history", []))

    verdict_count = state.get("verdict_count", 0) + 1
    print(f"\n[N3] Reviewing draft (review #{verdict_count})...")

    # Use mock provider in mock mode, otherwise use configured reviewer
    if mock_mode:
        reviewer_spec = "mock:review"
    else:
        reviewer_spec = state.get("config_reviewer", "gemini:3-pro-preview")

    # Determine review prompt path based on workflow type
    if workflow_type == "issue":
        prompt_path = Path("docs/skills/0701c-Issue-Review-Prompt.md")
    else:
        prompt_path = Path("docs/skills/0702c-LLD-Review-Prompt.md")

    # Load review prompt
    try:
        review_prompt = load_review_prompt(prompt_path, agentos_root)
    except FileNotFoundError as e:
        return {"error_message": str(e)}

    # Get reviewer provider
    try:
        reviewer = get_provider(reviewer_spec)
    except ValueError as e:
        return {"error_message": f"Invalid reviewer: {e}"}

    # System prompt for reviewing
    system_prompt = """You are a Principal Architect, Systems Engineer, and Test Plan Execution Guru.

Your role is to perform a strict gatekeeper review of design documents before implementation begins.

Key responsibilities:
- Answer any open questions in Section 1 with concrete recommendations
- Evaluate cost, safety, security, and legal concerns
- Verify test coverage meets requirements
- Provide a structured verdict: APPROVED or BLOCKED

Follow the Review Instructions exactly. Be specific about what needs to change for BLOCKED verdicts."""

    # Build review content
    review_content = f"""## Document to Review

{current_draft}

## Review Instructions

{review_prompt}"""

    # Call reviewer
    print(f"    Reviewer: {reviewer_spec}")
    result = reviewer.invoke(system_prompt=system_prompt, content=review_content)

    if not result.success:
        print(f"    ERROR: {result.error_message}")
        return {"error_message": f"Reviewer failed: {result.error_message}"}

    verdict_content = result.response or ""

    # Save to audit trail
    file_num = next_file_number(audit_dir)
    if audit_dir.exists():
        verdict_path = save_audit_file(
            audit_dir, file_num, "verdict.md", verdict_content
        )
    else:
        verdict_path = None

    # Append to verdict history
    verdict_history.append(verdict_content)

    # Determine LLD status from verdict
    lld_status = _parse_verdict_status(verdict_content)

    # Issue #248: Check open questions resolution status
    open_questions_status = _check_open_questions_status(current_draft, verdict_content)

    verdict_lines = len(verdict_content.splitlines()) if verdict_content else 0
    print(f"    Verdict: {lld_status} ({verdict_lines} lines)")
    print(f"    Open Questions: {open_questions_status}")
    if verdict_path:
        print(f"    Saved: {verdict_path.name}")

    return {
        "current_verdict": verdict_content,
        "current_verdict_path": str(verdict_path) if verdict_path else "",
        "verdict_count": verdict_count,
        "verdict_history": verdict_history,
        "file_counter": file_num,
        "lld_status": lld_status,
        "open_questions_status": open_questions_status,
        "error_message": "",
    }


def _parse_verdict_status(verdict_content: str) -> str:
    """Parse LLD status from verdict content.

    Args:
        verdict_content: The reviewer's verdict text.

    Returns:
        One of: "APPROVED", "BLOCKED"
    """
    verdict_upper = verdict_content.upper()

    # Check for checked APPROVED checkbox
    if re.search(r"\[X\]\s*\**APPROVED\**", verdict_upper):
        return "APPROVED"
    # Check for checked REVISE checkbox (maps to BLOCKED for workflow purposes)
    elif re.search(r"\[X\]\s*\**REVISE\**", verdict_upper):
        return "BLOCKED"
    # Check for checked DISCUSS checkbox (maps to BLOCKED, needs human)
    elif re.search(r"\[X\]\s*\**DISCUSS\**", verdict_upper):
        return "BLOCKED"
    # Fallback: Look for explicit keywords (legacy/simple responses)
    elif "VERDICT: APPROVED" in verdict_upper:
        return "APPROVED"
    elif "VERDICT: BLOCKED" in verdict_upper or "VERDICT: REVISE" in verdict_upper:
        return "BLOCKED"
    else:
        # Default to BLOCKED if we can't determine status (safe choice)
        return "BLOCKED"


def _check_open_questions_status(draft_content: str, verdict_content: str) -> str:
    """Check whether open questions have been resolved.

    Issue #248: After Gemini review, check if:
    1. Questions were answered (all [x] in verdict's "Open Questions Resolved" section)
    2. Questions marked as HUMAN REQUIRED
    3. Questions remain unanswered

    Args:
        draft_content: The draft that was reviewed.
        verdict_content: Gemini's verdict.

    Returns:
        One of:
        - "RESOLVED": All open questions answered
        - "HUMAN_REQUIRED": One or more questions need human decision
        - "UNANSWERED": Questions exist but weren't answered
        - "NONE": No open questions in the draft
    """
    # Check if draft has open questions
    draft_has_questions = _draft_has_open_questions(draft_content)
    if not draft_has_questions:
        return "NONE"

    # Check for HUMAN REQUIRED in verdict
    if _verdict_has_human_required(verdict_content):
        return "HUMAN_REQUIRED"

    # Check if verdict has "Open Questions Resolved" section with answers
    if _verdict_has_resolved_questions(verdict_content):
        return "RESOLVED"

    # Questions exist but weren't answered
    return "UNANSWERED"


def _draft_has_open_questions(content: str) -> bool:
    """Check if draft has unchecked open questions.

    Args:
        content: Draft content.

    Returns:
        True if unchecked open questions exist.
    """
    if not content:
        return False

    # Extract Open Questions section
    pattern = r"(?:^##?#?\s*Open Questions\s*\n)(.*?)(?=^##|\Z)"
    match = re.search(pattern, content, re.MULTILINE | re.DOTALL)

    if not match:
        return False

    open_questions_section = match.group(1)

    # Check for unchecked boxes
    unchecked = re.findall(r"^- \[ \]", open_questions_section, re.MULTILINE)
    return len(unchecked) > 0


def _verdict_has_human_required(verdict_content: str) -> bool:
    """Check if verdict contains HUMAN REQUIRED marker.

    Args:
        verdict_content: The verdict text.

    Returns:
        True if HUMAN REQUIRED is present.
    """
    # Look for HUMAN REQUIRED (case insensitive) in various formats
    patterns = [
        r"HUMAN\s+REQUIRED",
        r"\*\*HUMAN\s+REQUIRED\*\*",
        r"REQUIRES?\s+HUMAN",
        r"NEEDS?\s+HUMAN\s+DECISION",
        r"ESCALATE\s+TO\s+HUMAN",
    ]
    verdict_upper = verdict_content.upper()
    for pattern in patterns:
        if re.search(pattern, verdict_upper):
            return True
    return False


def _verdict_has_resolved_questions(verdict_content: str) -> bool:
    """Check if verdict has resolved open questions.

    Looks for the "Open Questions Resolved" section and checks if
    all items are marked as [x] with RESOLVED.

    Args:
        verdict_content: The verdict text.

    Returns:
        True if questions were resolved.
    """
    # Look for "Open Questions Resolved" section
    pattern = r"(?:##\s*Open Questions Resolved\s*\n)(.*?)(?=^##|\Z)"
    match = re.search(pattern, verdict_content, re.MULTILINE | re.DOTALL)

    if not match:
        # No explicit section - check if "RESOLVED:" appears in verdict
        return "RESOLVED:" in verdict_content.upper()

    resolved_section = match.group(1)

    # Check for resolved markers: [x] followed by ~~question~~ **RESOLVED:
    resolved_count = len(re.findall(r"\[x\].*?RESOLVED:", resolved_section, re.IGNORECASE))

    # Check for any unchecked items still in the section
    unchecked_count = len(re.findall(r"^- \[ \]", resolved_section, re.MULTILINE))

    # If we have resolutions and no unchecked items, questions are resolved
    return resolved_count > 0 and unchecked_count == 0
```

#### agentos/workflows/requirements/graph.py (Modify)

Add conditional edge for question-loop after review

```python
"""Parameterized StateGraph for Requirements Workflow.

Issue #101: Unified Requirements Workflow
Issue #248: Add conditional edge for question-loop after review

Creates a LangGraph StateGraph that connects:
- N0: load_input (brief or issue loading)
- N1: generate_draft (pluggable drafter)
- N2: human_gate_draft (human checkpoint)
- N3: review (pluggable reviewer)
- N4: human_gate_verdict (human checkpoint)
- N5: finalize (issue filing or LLD saving)

Graph structure:
    START -> N0 -> N1 -> N2 -> N3 -> N4 -> N5 -> END
                    ^          |         |
                    |          v         |
                    +-----<----+---------+

Issue #248 addition: After N3 (review), if open questions are UNANSWERED,
loop back to N3 with a followup prompt. If HUMAN_REQUIRED, force N4.

Routing is controlled by:
- error_message: Non-empty routes to END
- config_gates_*: Whether human gates are enabled
- next_node: Set by human gate nodes for routing decisions
- lld_status: Used for auto-routing when gates disabled
- open_questions_status: Used for question-loop routing (Issue #248)
"""

from typing import Literal

from langgraph.graph import END, START, StateGraph

from agentos.workflows.requirements.nodes import (
    finalize,
    generate_draft,
    human_gate_draft,
    human_gate_verdict,
    load_input,
    review,
)
from agentos.workflows.requirements.state import RequirementsWorkflowState


# =============================================================================
# Node Name Constants
# =============================================================================

N0_LOAD_INPUT = "N0_load_input"
N1_GENERATE_DRAFT = "N1_generate_draft"
N2_HUMAN_GATE_DRAFT = "N2_human_gate_draft"
N3_REVIEW = "N3_review"
N4_HUMAN_GATE_VERDICT = "N4_human_gate_verdict"
N5_FINALIZE = "N5_finalize"


# =============================================================================
# Routing Functions
# =============================================================================


def route_after_load_input(
    state: RequirementsWorkflowState,
) -> Literal["N1_generate_draft", "END"]:
    """Route after load_input node.

    Routes to:
    - N1_generate_draft: Success
    - END: Error loading input

    Args:
        state: Current workflow state.

    Returns:
        Next node name.
    """
    if state.get("error_message"):
        return "END"
    return "N1_generate_draft"


def route_after_generate_draft(
    state: RequirementsWorkflowState,
) -> Literal["N2_human_gate_draft", "N3_review", "END"]:
    """Route after generate_draft node.

    Routes to:
    - N2_human_gate_draft: Gate enabled
    - N3_review: Gate disabled
    - END: Error generating draft

    Issue #248: Pre-review validation gate removed. Drafts with open
    questions now proceed to review where Gemini can answer them.

    Args:
        state: Current workflow state.

    Returns:
        Next node name.
    """
    if state.get("error_message"):
        return "END"

    if state.get("config_gates_draft", True):
        return "N2_human_gate_draft"
    else:
        return "N3_review"


def route_from_human_gate_draft(
    state: RequirementsWorkflowState,
) -> Literal["N3_review", "N1_generate_draft", "END"]:
    """Route from human_gate_draft node.

    Routes based on next_node set by the gate:
    - N3_review: Send to review
    - N1_generate_draft: Revise draft
    - END: Manual handling

    Args:
        state: Current workflow state.

    Returns:
        Next node name.
    """
    next_node = state.get("next_node", "")

    if next_node == "N3_review":
        return "N3_review"
    elif next_node == "N1_generate_draft":
        return "N1_generate_draft"
    else:
        return "END"


def route_after_review(
    state: RequirementsWorkflowState,
) -> Literal["N4_human_gate_verdict", "N5_finalize", "N1_generate_draft", "N3_review", "END"]:
    """Route after review node.

    Issue #248: Extended routing for open questions loop.

    Routes to:
    - N4_human_gate_verdict: Gate enabled OR open questions HUMAN_REQUIRED
    - N5_finalize: Gate disabled and approved
    - N1_generate_draft: Gate disabled and blocked (if iterations remain)
    - N3_review: Open questions UNANSWERED (loop back for followup)
    - END: Error in review or max iterations reached

    Args:
        state: Current workflow state.

    Returns:
        Next node name.
    """
    if state.get("error_message"):
        return "END"

    # Issue #248: Check open questions status first
    open_questions_status = state.get("open_questions_status", "NONE")

    # If HUMAN_REQUIRED, force human gate regardless of gate config
    if open_questions_status == "HUMAN_REQUIRED":
        print("    [ROUTING] Open questions marked HUMAN REQUIRED - escalating to human gate")
        return "N4_human_gate_verdict"

    # If UNANSWERED, loop back to review (not draft - this is a review followup)
    # But respect max_iterations to prevent infinite loops
    if open_questions_status == "UNANSWERED":
        iteration_count = state.get("iteration_count", 0)
        max_iterations = state.get("max_iterations", 20)
        if iteration_count >= max_iterations:
            print(f"    [ROUTING] Max iterations ({max_iterations}) reached with unanswered questions - going to human gate")
            return "N4_human_gate_verdict"
        print("    [ROUTING] Open questions unanswered - looping back to drafter for revision")
        return "N1_generate_draft"

    # Normal routing
    if state.get("config_gates_verdict", True):
        return "N4_human_gate_verdict"
    else:
        # Auto-route based on verdict
        lld_status = state.get("lld_status", "PENDING")
        if lld_status == "APPROVED":
            return "N5_finalize"
        else:
            # Check max iterations before looping back
            iteration_count = state.get("iteration_count", 0)
            max_iterations = state.get("max_iterations", 20)
            if iteration_count >= max_iterations:
                # Max iterations reached - finalize with current status
                return "N5_finalize"
            return "N1_generate_draft"


def route_from_human_gate_verdict(
    state: RequirementsWorkflowState,
) -> Literal["N5_finalize", "N1_generate_draft", "END"]:
    """Route from human_gate_verdict node.

    Routes based on next_node set by the gate:
    - N5_finalize: Approve and finalize
    - N1_generate_draft: Revise draft
    - END: Manual handling

    Args:
        state: Current workflow state.

    Returns:
        Next node name.
    """
    next_node = state.get("next_node", "")

    if next_node == "N5_finalize":
        return "N5_finalize"
    elif next_node == "N1_generate_draft":
        return "N1_generate_draft"
    else:
        return "END"


def route_after_finalize(
    state: RequirementsWorkflowState,
) -> Literal["END"]:
    """Route after finalize node.

    Always routes to END (workflow complete).

    Args:
        state: Current workflow state.

    Returns:
        END.
    """
    return "END"


# =============================================================================
# Graph Creation
# =============================================================================


def create_requirements_graph() -> StateGraph:
    """Create the requirements workflow graph.

    Graph structure:
        START -> N0 -> N1 -> N2 -> N3 -> N4 -> N5 -> END
                        ^          |         |
                        |          v         |
                        +-----<----+---------+

    Issue #248: N3 can now loop back to N1 when open questions are
    unanswered, or force N4 when questions require human decision.

    Returns:
        Uncompiled StateGraph.
    """
    # Create graph with state schema
    graph = StateGraph(RequirementsWorkflowState)

    # Add nodes
    graph.add_node(N0_LOAD_INPUT, load_input)
    graph.add_node(N1_GENERATE_DRAFT, generate_draft)
    graph.add_node(N2_HUMAN_GATE_DRAFT, human_gate_draft)
    graph.add_node(N3_REVIEW, review)
    graph.add_node(N4_HUMAN_GATE_VERDICT, human_gate_verdict)
    graph.add_node(N5_FINALIZE, finalize)

    # Add edges
    # START -> N0
    graph.add_edge(START, N0_LOAD_INPUT)

    # N0 -> N1 or END (on error)
    graph.add_conditional_edges(
        N0_LOAD_INPUT,
        route_after_load_input,
        {
            "N1_generate_draft": N1_GENERATE_DRAFT,
            "END": END,
        },
    )

    # N1 -> N2 or N3 or END (based on gates and error)
    graph.add_conditional_edges(
        N1_GENERATE_DRAFT,
        route_after_generate_draft,
        {
            "N2_human_gate_draft": N2_HUMAN_GATE_DRAFT,
            "N3_review": N3_REVIEW,
            "END": END,
        },
    )

    # N2 -> N3 or N1 or END (based on human decision)
    graph.add_conditional_edges(
        N2_HUMAN_GATE_DRAFT,
        route_from_human_gate_draft,
        {
            "N3_review": N3_REVIEW,
            "N1_generate_draft": N1_GENERATE_DRAFT,
            "END": END,
        },
    )

    # N3 -> N4 or N5 or N1 or N3 or END (based on gates, verdict, and open questions)
    # Issue #248: Added N3_review as possible target for open questions loop
    graph.add_conditional_edges(
        N3_REVIEW,
        route_after_review,
        {
            "N4_human_gate_verdict": N4_HUMAN_GATE_VERDICT,
            "N5_finalize": N5_FINALIZE,
            "N1_generate_draft": N1_GENERATE_DRAFT,
            "N3_review": N3_REVIEW,
            "END": END,
        },
    )

    # N4 -> N5 or N1 or END (based on human decision)
    graph.add_conditional_edges(
        N4_HUMAN_GATE_VERDICT,
        route_from_human_gate_verdict,
        {
            "N5_finalize": N5_FINALIZE,
            "N1_generate_draft": N1_GENERATE_DRAFT,
            "END": END,
        },
    )

    # N5 -> END
    graph.add_edge(N5_FINALIZE, END)

    return graph
```

#### docs/skills/0702c-LLD-Review-Prompt.md (Modify)

Add Open Questions answering instructions

```python
# 0702c - LLD Review Prompt (Golden Schema v2.0)

## Metadata

| Field | Value |
|-------|-------|
| **Version** | 2.5.0 |
| **Last Updated** | 2026-02-03 |
| **Role** | Senior Software Architect & AI Governance Lead |
| **Purpose** | LLD gatekeeper review before implementation begins |
| **Standard** | [0010-prompt-schema.md](../standards/0010-prompt-schema.md) |

---

## Critical Protocol

You are acting as a **Senior Software Architect & AI Governance Lead**. Your goal is to perform a strict, gatekeeper review of a Low-Level Design (LLD) document before implementation begins.

**CRITICAL INSTRUCTIONS:**

1. **Identity Handshake:** Begin your response by confirming your identity as Gemini 3 Pro.
2. **No Implementation:** Do NOT offer to write code, implement features, or fix issues yourself. Your role is strictly review and oversight.
3. **Strict Gating:** You must REJECT the LLD if Pre-Flight Gate fails OR if Tier 1 issues exist.

---

## Pre-Flight Gate (CHECK FIRST)

**Before reviewing the LLD content, verify these structural requirements:**

| Requirement | Check |
|-------------|-------|
| **GitHub Issue Link** | Does the LLD explicitly link to a specific GitHub Issue (e.g., `#47`)? |
| **Context/Scope Section** | Does the LLD contain a "Context" or "Scope" section defining the problem? |
| **Proposed Changes Section** | Does the LLD contain a "Proposed Changes" or "Design" section? |

**If ANY requirement is missing:** STOP reviewing and output:

```markdown
## Pre-Flight Gate: FAILED

The submitted LLD does not meet structural requirements for review.

### Missing Required Elements:
- [ ] {List each missing element}

**Verdict: REJECTED - LLD must include all required elements before review can proceed.**
```

---

## Open Questions Protocol

OPEN QUESTIONS:
- The draft may contain unchecked open questions in Section 1
- You MUST answer each question with a concrete recommendation
- Mark answered questions as [x] with your recommendation
- Format: `- [x] ~~Original question~~ **RESOLVED: Your answer.**`

---

## Tier 1: BLOCKING (Must Pass)

These issues PREVENT implementation from starting. Be exhaustive.

### Cost

| Check | Question |
|-------|----------|
| **Model Tier Selection** | Is the model tier appropriate for the task complexity? (REJECT Opus for simple CRUD operations, file manipulation, or straightforward refactors. Reserve Opus for complex architectural decisions.) |
| **Loop Bounds** | Are all loops explicitly bounded? Can infinite loops or runaway recursion occur? |
| **API Call Volume** | Does the design minimize API calls? Are batch operations used where appropriate? |
| **Token Budget** | For LLM-heavy operations, is there a token budget or limit defined? |

### Safety

| Check | Question |
|-------|----------|
| **Worktree Scope (CRITICAL)** | Does the design allow execution OUTSIDE the designated worktree? If yes, REJECT. All file operations must be scoped to the worktree. |
| **Destructive Acts (CRITICAL)** | Does the design involve destructive operations (delete, overwrite, force-push)? If yes, is explicit human confirmation REQUIRED before execution? |
| **Permission Friction** | Does this design introduce new permission prompts? (Reference: Audit 0815) If yes, document mitigation strategy. |
| **Fail-Safe Strategy** | Are timeout/failure paths explicitly defined? Is "Silent Failure" prevented? (Fail Open vs. Fail Closed must be specified.) |

### Security

| Check | Question |
|-------|----------|
| **Secrets Management** | Are credentials, API keys, or tokens involved? Is secure handling specified (environment variables, keychain, NOT hardcoded)? |
| **Input Validation** | Is all external input validated and sanitized? Injection risks addressed? |
| **OWASP Top 10** | Are common vulnerabilities addressed (XSS, SQLi, CSRF, auth bypass)? |

### Legal

| Check | Question |
|-------|----------|
| **Privacy & Data Residency** | Does the design handle PII? Is data processed locally only? Is GDPR/CCPA compliance addressed? |
| **License Compliance** | Are new dependencies introduced? Are their licenses compatible (MIT, Apache 2.0, BSD)? |
| **Toxic Content Logging** | Does the design prevent logging of sensitive/toxic content? |

---

## Tier 2: HIGH PRIORITY (Should Pass)

These issues require fixes but don't block implementation. Be thorough.

### Architecture

| Check | Question |
|-------|----------|
| **Path Structure (CRITICAL)** | Do file paths in "Files Changed" match the actual project directory structure? **BLOCK if LLD uses `src/module/` when project uses `module/` (or vice versa).** Validate against existing codebase layout. Common error: LLDs defaulting to `src/` prefix when project doesn't use it. |
| **Design Patterns** | Does the design follow established project patterns? |
| **Dependency Chain** | Are blocking dependencies and parallel work identified? |
| **Offline Development (CRITICAL)** | Can this be developed "on an airplane"? Is a Mock Mode defined for external dependencies (APIs, Auth, LLMs)? |
| **Interface Correctness** | Do proposed interfaces match existing contracts? Are edge cases handled? |

### Observability

| Check | Question |
|-------|----------|
| **LangSmith Tracing** | For agent operations, is LangSmith tracing configured? Are trace IDs propagated? |
| **Logging Strategy** | Are key operations logged at appropriate levels? Can issues be debugged from logs alone? |
| **Metrics Collection** | Are relevant metrics identified for dashboarding (latency, success rate, cost)? |

### Quality

| Check | Question |
|-------|----------|
| **Section 10.0 TDD Test Plan (CRITICAL)** | Does Section 10.0 contain a Test Plan table with Test ID, Description, Expected Behavior, and Status columns? Are tests marked as RED (not yet implemented)? Is coverage target ≥95% specified? LLDs without a TDD test plan BLOCK implementation. |
| **Section 10 Test Scenarios (CRITICAL)** | Does Section 10 contain a structured table of test scenarios with columns for: ID/Name, Scenario/Description, Type (unit/integration/e2e), and Expected behavior? LLDs without parseable test scenarios BLOCK the TDD workflow. |
| **Requirement Coverage (CRITICAL - 95% threshold)** | You MUST output a **Requirement Coverage Table** (see Output Format below). Extract EVERY numbered requirement from Section 3. Map each to test scenario(s) from Section 10. Calculate coverage = (requirements with tests / total requirements). **BLOCK if coverage < 95%.** Do NOT make qualitative assessments - count explicitly. |
| **Test Assertions (CRITICAL)** | Does every test scenario have explicit assertions or expected outcomes? **BLOCK if any test is vague** (e.g., "verify it works", "check behavior", "test the feature"). Each test must specify WHAT is checked and WHAT the expected result is. |
| **No Human Delegation (CRITICAL)** | Do any tests delegate to human verification? **BLOCK if any test says:** "manual verification", "visual check", "observe behavior", "ask user", "human review", or requires judgment to determine pass/fail. ALL tests must be fully automated. |
| **Test Strategy (CRITICAL)** | Is the test strategy defined? Does it rely on automated assertions, NOT manual "vibes" verification? |
| **Willison Protocol** | Will tests fail if the implementation is reverted? (Tests must prove the feature works, not just that it doesn't crash.) |
| **Edge Cases** | Are edge cases covered: empty inputs, invalid inputs, boundary conditions, error conditions? (Warning if missing, not blocking.) |
| **Test Data Hygiene** | Are test fixtures defined? No real PII or slurs in test data? |
| **Scope Boundaries** | Is the scope bounded to prevent creep? |

---

## Tier 3: SUGGESTIONS (Nice to Have)

Note these but don't block on them.

| Check | Question |
|-------|----------|
| **Performance** | Are latency/memory budgets defined? |
| **Maintainability** | Is the code structure clear for future agent readability? |
| **Documentation** | Is the LLD complete per the project template? |
| **Extensibility** | Does the design allow for future enhancements without major refactoring? |

---

## Output Format (Strictly Follow This)

```markdown
# LLD Review: {IssueID}-{title}

## Identity Confirmation
I am Gemini 3 Pro, acting as Senior Software Architect & AI Governance Lead.

## Pre-Flight Gate
{PASSED or FAILED with missing elements listed}

## Review Summary
{2-3 sentence overall assessment of the LLD's readiness for implementation}

## Open Questions Resolved
{If Section 1 had unchecked questions, list your answers here}
- [x] ~~Question 1~~ **RESOLVED: Your answer.**
- [x] ~~Question 2~~ **RESOLVED: Your answer.**
{If no open questions, write "No open questions found in Section 1."}

## Requirement Coverage Analysis (MANDATORY)

**Section 3 Requirements:**
| # | Requirement | Test(s) | Status |
|---|-------------|---------|--------|
| 1 | {Requirement text from Section 3} | {test_XXX, test_YYY} | ✓ Covered |
| 2 | {Requirement text} | - | **GAP** |
{List ALL requirements from Section 3 - do not skip any}

**Coverage Calculation:** {X} requirements covered / {Y} total = **{Z}%**

**Verdict:** {PASS if ≥95%, BLOCK if <95%}

{If any GAP exists, list the missing test scenarios that must be added}

## Tier 1: BLOCKING Issues
{If none, write "No blocking issues found. LLD is approved for implementation."}

### Cost
- [ ] {Issue description + recommendation}

### Safety
- [ ] {Issue description + recommendation}

### Security
- [ ] {Issue description + recommendation}

### Legal
- [ ] {Issue description + recommendation}

## Tier 2: HIGH PRIORITY Issues
{If none, write "No high-priority issues found."}

### Architecture
- [ ] {Issue description + recommendation}

### Observability
- [ ] {Issue description + recommendation}

### Quality
- [ ] {Issue description + recommendation}
- [ ] **Requirement Coverage:** {PASS or BLOCK based on coverage calculation above. If BLOCK, this is a Tier 2 issue that prevents approval.}

## Tier 3: SUGGESTIONS
{Brief bullet points only}
- {Suggestion}

## Questions for Orchestrator
1. {Question requiring human judgment, if any}

## Verdict
[ ] **APPROVED** - Ready for implementation
[ ] **REVISE** - Fix Tier 1/2 issues first
[ ] **DISCUSS** - Needs Orchestrator decision
```

---

## Example: Pre-Flight Gate Failure

```markdown
# LLD Review: Feature-Export-Dashboard

## Identity Confirmation
I am Gemini 3 Pro, acting as Senior Software Architect & AI Governance Lead.

## Pre-Flight Gate: FAILED

The submitted LLD does not meet structural requirements for review.

### Missing Required Elements:
- [ ] GitHub Issue Link - No issue reference found (e.g., "Implements #47")
- [ ] Context/Scope Section - No problem statement or scope definition

**Verdict: REJECTED - LLD must include all required elements before review can proceed.**
```

---

## Example: Tier 1 Safety Block

```markdown
# LLD Review: #52-Batch-File-Cleanup

## Identity Confirmation
I am Gemini 3 Pro, acting as Senior Software Architect & AI Governance Lead.

## Pre-Flight Gate: PASSED
All required elements present.

## Review Summary
The LLD proposes a batch file cleanup utility but contains critical Safety blockers. The design allows operations outside the worktree and lacks human confirmation for destructive acts. Cannot proceed until these are addressed.

## Tier 1: BLOCKING Issues

### Cost
- [ ] **Loop bounds undefined:** The `for file in files` loop has no upper limit. Add: `max_files = 1000` with explicit handling if exceeded.

### Safety
- [ ] **CRITICAL - Worktree Scope Violation:** Design allows deletion of files in `~/Downloads/` which is OUTSIDE the worktree. All operations MUST be scoped to the project worktree only.
- [ ] **CRITICAL - No Human Confirmation:** Batch deletion of files requires explicit human confirmation before execution. Add: `confirm_destructive_action()` gate.

### Security
- [ ] No issues found.

### Legal
- [ ] No issues found.

## Tier 2: HIGH PRIORITY Issues

### Architecture
- [ ] No issues found.

### Observability
- [ ] **Missing logging:** No logging defined for deleted files. Add audit log of all deletions.

### Quality
- [ ] **Test strategy incomplete:** Tests should verify that files OUTSIDE worktree are NOT touched.

## Tier 3: SUGGESTIONS
- Consider dry-run mode as default behavior
- Add `--force` flag requirement for actual deletions

## Questions for Orchestrator
1. Should there be a file count threshold that requires Orchestrator approval (e.g., >100 files)?

## Verdict
[ ] **APPROVED** - Ready for implementation
[x] **REVISE** - Fix Tier 1/2 issues first
[ ] **DISCUSS** - Needs Orchestrator decision
```

---

## History

| Date | Version | Change |
|------|---------|--------|
| 2026-02-03 | 2.5.0 | Added Open Questions Protocol. Gemini must answer unchecked questions in Section 1 before approving. Issue #248. |
| 2026-02-03 | 2.4.0 | Added TDD Test Plan check (Section 10.0) to Quality tier. Verifies tests are marked RED before implementation and coverage target ≥95% is specified. Issue #209. |
| 2026-02-02 | 2.3.0 | Added MANDATORY Requirement Coverage Table to output format. Gemini must explicitly map every Section 3 requirement to tests and calculate coverage numerically. Fixes LLD-141 gap where Gemini saw missing test but classified as Tier 3 suggestion instead of blocking. |
| 2026-02-02 | 2.2.0 | Added Path Structure check to Architecture tier. BLOCKS if LLD file paths don't match actual project structure (e.g., `src/module/` vs `module/`). Issue #106 root cause. |
| 2026-02-02 | 2.1.0 | Added 0706c test plan checks: 95% coverage, explicit assertions, no human delegation, edge cases (#126). Unified LLD review with test plan review. |
| 2026-01-22 | 2.0.0 | Refactored to Golden Schema (Standard 0010). Added Pre-Flight Gate, Cost/Safety tiers, Observability section. |
| 2026-01-XX | 1.0.0 | Initial version. |

```

#### tests/unit/test_open_questions_loop.py (NEW FILE)

Unit tests for new loop behavior

### Previous Test Run (FAILED)

The previous implementation attempt failed. Here's the test output:

```
ertionError: TDD: Implementati...
FAILED tests/test_issue_248.py::test_t040 - AssertionError: TDD: Implementati...
FAILED tests/test_issue_248.py::test_t050 - AssertionError: TDD: Implementati...
FAILED tests/test_issue_248.py::test_t060 - AssertionError: TDD: Implementati...
FAILED tests/test_issue_248.py::test_t070 - AssertionError: TDD: Implementati...
FAILED tests/test_issue_248.py::test_010 - AssertionError: TDD: Implementatio...
FAILED tests/test_issue_248.py::test_020 - AssertionError: TDD: Implementatio...
FAILED tests/test_issue_248.py::test_030 - AssertionError: TDD: Implementatio...
FAILED tests/test_issue_248.py::test_040 - AssertionError: TDD: Implementatio...
FAILED tests/test_issue_248.py::test_050 - AssertionError: TDD: Implementatio...
FAILED tests/test_issue_248.py::test_060 - AssertionError: TDD: Implementatio...
FAILED tests/test_issue_248.py::test_070 - AssertionError: TDD: Implementatio...
============================= 15 failed in 0.07s ==============================

C:\Users\mcwiz\AppData\Local\pypoetry\Cache\virtualenvs\unleashed-Zukdy2xA-py3.14\Lib\site-packages\coverage\inorout.py:497: CoverageWarning: Module agentos.workflows.requirements.nodes.generate_draft was never imported. (module-not-imported); see https://coverage.readthedocs.io/en/7.13.1/messages.html#warning-module-not-imported
  self.warn(f"Module {pkg} was never imported.", slug="module-not-imported")
C:\Users\mcwiz\AppData\Local\pypoetry\Cache\virtualenvs\unleashed-Zukdy2xA-py3.14\Lib\site-packages\coverage\control.py:958: CoverageWarning: No data was collected. (no-data-collected); see https://coverage.readthedocs.io/en/7.13.1/messages.html#warning-no-data-collected
  self._warn("No data was collected.", slug="no-data-collected")
C:\Users\mcwiz\AppData\Local\pypoetry\Cache\virtualenvs\unleashed-Zukdy2xA-py3.14\Lib\site-packages\pytest_cov\plugin.py:363: CovReportWarning: Failed to generate report: No data to report.

  warnings.warn(CovReportWarning(message), stacklevel=1)

```

Please fix the issues and provide updated implementation.

## Instructions

1. Generate implementation code that makes all tests pass
2. Follow the patterns established in the codebase
3. Ensure proper error handling
4. Add type hints where appropriate
5. Keep the implementation minimal - only what's needed to pass tests

## Output Format (CRITICAL - MUST FOLLOW EXACTLY)

For EACH file you need to create or modify, provide a code block with this EXACT format:

```python
# File: path/to/implementation.py

def function_name():
    ...
```

**Rules:**
- The `# File: path/to/file` comment MUST be the FIRST line inside the code block
- Use the language-appropriate code fence (```python, ```gitignore, ```yaml, etc.)
- Path must be relative to repository root (e.g., `src/module/file.py`)
- Do NOT include "(append)" or other annotations in the path
- Provide complete file contents, not patches or diffs

**Example for .gitignore:**
```gitignore
# File: .gitignore

# Existing patterns...
*.pyc
__pycache__/

# New pattern
.agentos/
```

If multiple files are needed, provide each in a separate code block with its own `# File:` header.
