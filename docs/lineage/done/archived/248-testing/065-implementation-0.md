# File: tests/test_issue_248.py

```python
"""Test file for Issue #248.

Generated by AgentOS TDD Testing Workflow.
Tests verify the open questions loop behavior where Gemini answers
questions before human escalation.
"""

import pytest
import re
from pathlib import Path
from unittest.mock import Mock, patch


# Unit Tests
# -----------

def test_id():
    """
    Verify that workflow state includes open_questions_status field.
    """
    from agentos.workflows.requirements.state import create_initial_state

    state = create_initial_state(
        workflow_type="lld",
        agentos_root="/tmp/agentos",
        target_repo="/tmp/repo",
        issue_number=248,
    )

    # Verify field exists and has default value
    assert "open_questions_status" in state
    assert state["open_questions_status"] == "NONE"


def test_t010():
    """
    test_draft_with_questions_proceeds_to_review | Draft not blocked
    pre-review | RED
    """
    from agentos.workflows.requirements.graph import route_after_generate_draft

    # Draft with open questions should proceed to review (gate disabled)
    # Pre-review validation was removed per Issue #248
    state = {
        "error_message": "",
        "config_gates_draft": False,
    }

    result = route_after_generate_draft(state)
    
    # Should route to review, not block
    assert result == "N3_review"


def test_t020():
    """
    test_gemini_answers_questions | Questions resolved in verdict | RED
    """
    from agentos.workflows.requirements.nodes.review import _check_open_questions_status

    draft = """# LLD
### Open Questions
- [ ] Which database should we use?
- [ ] What is the caching strategy?
"""

    verdict = """## Open Questions Resolved
- [x] ~~Which database should we use?~~ **RESOLVED: Use PostgreSQL.**
- [x] ~~What is the caching strategy?~~ **RESOLVED: Use Redis.**

## Verdict
[x] APPROVED"""

    status = _check_open_questions_status(draft, verdict)
    assert status == "RESOLVED"


def test_t030():
    """
    test_unanswered_triggers_loop | Loop back to N3 with followup | RED
    """
    from agentos.workflows.requirements.graph import route_after_review

    # When questions are unanswered, loop back to drafter
    state = {
        "error_message": "",
        "config_gates_verdict": False,
        "lld_status": "APPROVED",
        "open_questions_status": "UNANSWERED",
        "iteration_count": 5,
        "max_iterations": 20,
    }

    result = route_after_review(state)
    
    # Should loop back to N1 for revision with questions context
    assert result == "N1_generate_draft"


def test_t040():
    """
    test_human_required_escalates | Goes to human gate | RED
    """
    from agentos.workflows.requirements.graph import route_after_review

    # HUMAN_REQUIRED should force human gate
    state = {
        "error_message": "",
        "config_gates_verdict": False,  # Even with gates disabled
        "lld_status": "APPROVED",
        "open_questions_status": "HUMAN_REQUIRED",
    }

    result = route_after_review(state)
    
    assert result == "N4_human_gate_verdict"


def test_t050():
    """
    test_max_iterations_respected | Terminates after limit | RED
    """
    from agentos.workflows.requirements.graph import route_after_review

    # At max iterations with unanswered questions, go to human gate
    state = {
        "error_message": "",
        "config_gates_verdict": False,
        "lld_status": "BLOCKED",
        "open_questions_status": "UNANSWERED",
        "iteration_count": 20,
        "max_iterations": 20,
    }

    result = route_after_review(state)
    
    # Should escalate to human gate instead of infinite loop
    assert result == "N4_human_gate_verdict"


def test_t060():
    """
    test_all_answered_proceeds_to_finalize | N5 reached when resolved |
    RED
    """
    from agentos.workflows.requirements.graph import route_after_review

    # When questions resolved and approved, proceed to finalize
    state = {
        "error_message": "",
        "config_gates_verdict": False,
        "lld_status": "APPROVED",
        "open_questions_status": "RESOLVED",
    }

    result = route_after_review(state)
    
    assert result == "N5_finalize"


def test_t070():
    """
    test_prompt_includes_question_instructions | 0702c has new section |
    RED
    """
    # Check that 0702c prompt has the Open Questions Protocol
    possible_paths = [
        Path(__file__).parent.parent / "docs" / "skills" / "0702c-LLD-Review-Prompt.md",
        Path("docs/skills/0702c-LLD-Review-Prompt.md"),
        Path("C:/Users/mcwiz/Projects/AgentOS-248/docs/skills/0702c-LLD-Review-Prompt.md"),
    ]

    content = None
    for p in possible_paths:
        if p.exists():
            content = p.read_text()
            break

    if content is None:
        pytest.skip("Prompt file not found")

    # Verify Open Questions Protocol section
    assert "Open Questions Protocol" in content
    assert "RESOLVED:" in content
    assert "[x]" in content


def test_010():
    """
    Draft with open questions proceeds | Auto | Draft with 3 unchecked
    questions | Reaches N3_review | No BLOCKED status pre-review
    """
    from agentos.workflows.requirements.graph import route_after_generate_draft
    from agentos.workflows.requirements.nodes.generate_draft import validate_draft_structure

    # Validate that even drafts with open questions proceed
    draft_with_questions = """# LLD
### Open Questions
- [ ] Question 1?
- [ ] Question 2?
- [ ] Question 3?
"""

    # Pre-review validation exists but is no longer called in flow
    validation_result = validate_draft_structure(draft_with_questions)
    # It still returns a message for backward compatibility
    assert validation_result is not None  # Function still works

    # But routing proceeds regardless
    state = {
        "error_message": "",
        "config_gates_draft": False,
    }

    result = route_after_generate_draft(state)
    assert result == "N3_review"


def test_020():
    """
    Gemini answers questions | Auto | Review with question instructions |
    All questions [x] | Verdict contains resolutions
    """
    from agentos.workflows.requirements.nodes.review import _verdict_has_resolved_questions

    # Verdict with resolved questions section
    verdict = """## Open Questions Resolved
- [x] ~~Should we use JWT or session?~~ **RESOLVED: Use JWT for stateless auth.**
- [x] ~~What's the token expiry?~~ **RESOLVED: 24 hours with refresh.**

## Verdict
[x] APPROVED"""

    assert _verdict_has_resolved_questions(verdict) is True


def test_030():
    """
    Unanswered triggers loop | Auto | Verdict approves but questions
    unchecked | Loop to N3 | Followup prompt sent
    """
    from agentos.workflows.requirements.nodes.review import _check_open_questions_status
    from agentos.workflows.requirements.graph import route_after_review

    draft = """### Open Questions
- [ ] Unresolved question?"""

    verdict = "[x] APPROVED - Great design!"  # No resolution of questions

    status = _check_open_questions_status(draft, verdict)
    assert status == "UNANSWERED"

    # Then verify routing
    state = {
        "error_message": "",
        "config_gates_verdict": False,
        "lld_status": "APPROVED",
        "open_questions_status": "UNANSWERED",
        "iteration_count": 1,
        "max_iterations": 20,
    }

    result = route_after_review(state)
    assert result == "N1_generate_draft"


def test_040():
    """
    HUMAN REQUIRED escalates | Auto | Verdict with HUMAN REQUIRED | Goes
    to N4 | Human gate invoked
    """
    from agentos.workflows.requirements.nodes.review import _verdict_has_human_required
    from agentos.workflows.requirements.graph import route_after_review

    # Various HUMAN REQUIRED formats
    verdicts = [
        "HUMAN REQUIRED for this decision",
        "**HUMAN REQUIRED** - Business decision needed",
        "This REQUIRES HUMAN input",
        "NEEDS HUMAN DECISION on architecture",
        "ESCALATE TO HUMAN for approval",
    ]

    for verdict in verdicts:
        assert _verdict_has_human_required(verdict) is True, f"Failed for: {verdict}"

    # Routing should go to human gate
    state = {
        "error_message": "",
        "config_gates_verdict": False,
        "open_questions_status": "HUMAN_REQUIRED",
    }

    result = route_after_review(state)
    assert result == "N4_human_gate_verdict"


def test_050():
    """
    Max iterations respected | Auto | 20 loops without resolution |
    Terminates | Exit with current state
    """
    from agentos.workflows.requirements.graph import route_after_review

    # Test at various iteration counts
    # Under max - should loop
    state_under = {
        "error_message": "",
        "config_gates_verdict": False,
        "lld_status": "BLOCKED",
        "open_questions_status": "UNANSWERED",
        "iteration_count": 19,
        "max_iterations": 20,
    }
    assert route_after_review(state_under) == "N1_generate_draft"

    # At max - should go to human gate
    state_at_max = {
        "error_message": "",
        "config_gates_verdict": False,
        "lld_status": "BLOCKED",
        "open_questions_status": "UNANSWERED",
        "iteration_count": 20,
        "max_iterations": 20,
    }
    assert route_after_review(state_at_max) == "N4_human_gate_verdict"


def test_060():
    """
    Resolved proceeds to finalize | Auto | All questions answered |
    Reaches N5 | APPROVED status
    """
    from agentos.workflows.requirements.nodes.review import _check_open_questions_status
    from agentos.workflows.requirements.graph import route_after_review

    draft = """### Open Questions
- [ ] Question that was asked?"""

    verdict = """## Open Questions Resolved
- [x] ~~Question that was asked?~~ **RESOLVED: Use approach A.**

## Verdict
[x] APPROVED"""

    status = _check_open_questions_status(draft, verdict)
    assert status == "RESOLVED"

    state = {
        "error_message": "",
        "config_gates_verdict": False,
        "lld_status": "APPROVED",
        "open_questions_status": "RESOLVED",
    }

    result = route_after_review(state)
    assert result == "N5_finalize"


def test_070():
    """
    Prompt updated | Auto | Load 0702c | Contains question instructions |
    Regex match
    """
    possible_paths = [
        Path(__file__).parent.parent / "docs" / "skills" / "0702c-LLD-Review-Prompt.md",
        Path("docs/skills/0702c-LLD-Review-Prompt.md"),
        Path("C:/Users/mcwiz/Projects/AgentOS-248/docs/skills/0702c-LLD-Review-Prompt.md"),
    ]

    content = None
    for p in possible_paths:
        if p.exists():
            content = p.read_text()
            break

    if content is None:
        pytest.skip("Prompt file not found")

    # Check for key protocol elements
    assert "Open Questions Protocol" in content
    
    # Check format instructions
    assert re.search(r"\[x\].*RESOLVED:", content, re.IGNORECASE)
    assert "~~" in content  # Strikethrough format
    
    # Check it's in the output format section
    assert "Open Questions Resolved" in content
```