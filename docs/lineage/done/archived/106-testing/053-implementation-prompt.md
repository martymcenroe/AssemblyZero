# Implementation Request

## Context

You are implementing code for Issue #106 using TDD.
This is iteration 6 of the implementation.

## Requirements

The tests have been scaffolded and need implementation code to pass.

### LLD Summary

# 1106 - Feature: Parallel Workflow Execution for LLD and Issue Processing

<!-- Template Metadata
Last Updated: 2026-02-02
Updated By: Issue #117 fix
Update Reason: Moved Verification & Testing to Section 10 (was Section 11) to match 0702c review prompt and testing workflow expectations
Previous: Added sections based on 80 blocking issues from 164 governance verdicts (2026-02-01)
-->

## 1. Context & Goal
* **Issue:** #106
* **Objective:** Enable concurrent execution of LLD and Issue workflows with the `--all` flag, solving SQLite contention, credential pool management, and output coordination challenges to dramatically reduce batch operation time.
* **Status:** Draft
* **Related Issues:** N/A

### Open Questions
*Questions that need clarification before or during implementation. Remove when resolved.*

- [ ] Should rate-limited keys be automatically retried after backoff, or should manual intervention be required?
- [ ] What is the maximum acceptable coordination overhead percentage for parallel execution?

## 2. Proposed Changes

*This section is the **source of truth** for implementation. Describe exactly what will be built.*

### 2.1 Files Changed

| File | Change Type | Description |
|------|-------------|-------------|
| `agentos/workflows/parallel/__init__.py` | Add | Module exports for parallel workflow components |
| `agentos/workflows/parallel/coordinator.py` | Add | New coordinator managing worker pool, progress tracking, and graceful shutdown |
| `agentos/workflows/parallel/credential_coordinator.py` | Add | Thread-safe credential reservation system with rate-limit tracking |
| `agentos/workflows/parallel/output_prefixer.py` | Add | Stdout/stderr wrapper with prefix injection for workflow identification |
| `agentos/workflows/parallel/input_sanitizer.py` | Add | Input validation utilities for path-safe identifiers |
| `agentos/workflows/lld/workflow.py` | Modify | Add `--parallel` and `--dry-run` flags, integrate with coordinator |
| `agentos/workflows/...

### Test Scenarios

- **test_010**: Happy path: 3 LLDs processed in parallel | Auto | 3 mock LLDs, --parallel 3 | All complete, progress report shows 3/3 | Exit code 0, all DBs cleaned up
  - Requirement: 
  - Type: unit

- **test_020**: Dry run lists without executing | Auto | 5 pending items, --dry-run | List of 5 items printed | No subprocess spawned, no DBs created
  - Requirement: 
  - Type: unit

- **test_030**: Path traversal rejected | Auto | Issue number "../etc/passwd" | ValueError raised | Clear error message, no file access
  - Requirement: 
  - Type: unit

- **test_040**: Credential exhaustion pauses workers | Auto | 5 items, 2 credentials, --parallel 5 | Workers pause, resume on release | Log shows "[COORDINATOR] Credential pool exhausted"
  - Requirement: 
  - Type: unit

- **test_050**: HTTP 429 triggers backoff | Auto | AGENTOS_SIMULATE_429=true | Key marked rate-limited | Backoff applied, different key used or wait
  - Requirement: 
  - Type: unit

- **test_060**: Single workflow failure isolated | Auto | 1 invalid spec among 3 | 2 succeed, 1 fails | Failed item in report, others complete
  - Requirement: 
  - Type: unit

- **test_070**: Graceful shutdown on SIGINT | Auto | SIGINT during execution | Workers checkpoint and exit | All checkpoint DBs written within 5s
  - Requirement: 
  - Type: unit

- **test_080**: Output prefix prevents interleaving | Auto | 3 parallel workflows | All lines prefixed correctly | No partial line mixing
  - Requirement: 
  - Type: unit

- **test_090**: Performance benchmark | Auto-Live | 6 items, sequential vs --parallel 3 | Parallel < 50% sequential time | Timing comparison logged
  - Requirement: 
  - Type: unit

- **test_100**: Max parallelism enforced | Auto | Capped to 10 | Warning logged, runs with 10
  - Requirement: 
  - Type: unit

- **test_110**: Default parallelism applied | Auto | Uses 3 | Config shows max_parallelism=3
  - Requirement: 
  - Type: unit

### Test File: C:\Users\mcwiz\Projects\AgentOS-106\tests\test_issue_106.py

```python
"""Test file for Issue #106.

Generated by AgentOS TDD Testing Workflow.
Tests for parallel workflow execution infrastructure.
"""

import os
import signal
import time
import threading
from unittest.mock import Mock, patch, MagicMock
import pytest

from agentos.workflows.parallel import (
    ParallelCoordinator,
    CredentialCoordinator,
    OutputPrefixer,
    sanitize_identifier,
)
from agentos.workflows.parallel.coordinator import WorkflowResult, ProgressStats


# Fixtures for mocking
@pytest.fixture
def mock_external_service():
    """Mock external service for isolation."""
    yield None


@pytest.fixture
def mock_credentials():
    """Mock credentials for testing."""
    return ["key1", "key2", "key3"]


@pytest.fixture
def credential_coordinator(mock_credentials):
    """Create a credential coordinator with mock credentials."""
    return CredentialCoordinator(mock_credentials)


# Unit Tests
# -----------

def test_010(mock_credentials):
    """
    Happy path: 3 LLDs processed in parallel | Auto | 3 mock LLDs,
    --parallel 3 | All complete, progress report shows 3/3 | Exit code 0,
    all DBs cleaned up
    """
    # TDD: Arrange
    items = ["lld-001", "lld-002", "lld-003"]
    processed = []
    
    def worker_func(item, credential):
        processed.append(item)
        time.sleep(0.01)  # Simulate some work
        
    coordinator = ParallelCoordinator(max_workers=3)
    
    # TDD: Act
    stats, results = coordinator.execute_parallel(
        items=items,
        worker_func=worker_func,
        item_id_func=lambda x: x,
        dry_run=False,
    )
    
    # TDD: Assert
    assert stats.total == 3
    assert stats.completed == 3
    assert stats.failed == 0
    assert stats.success_count == 3
    assert len(results) == 3
    assert all(r.success for r in results)
    assert set(processed) == set(items)


def test_020():
    """
    Dry run lists without executing | Auto | 5 pending items, --dry-run |
    List of 5 items printed | No subprocess spawned, no DBs created
    """
    # TDD: Arrange
    items = ["item-001", "item-002", "item-003", "item-004", "item-005"]
    executed = []
    
    def worker_func(item, credential):
        executed.append(item)
        
    coordinator = ParallelCoordinator(max_workers=3)
    
    # TDD: Act
    with patch('builtins.print') as mock_print:
        stats, results = coordinator.execute_parallel(
            items=items,
            worker_func=worker_func,
            item_id_func=lambda x: x,
            dry_run=True,
        )
    
    # TDD: Assert
    assert stats.total == 5
    assert stats.completed == 0
    assert len(results) == 0
    assert len(executed) == 0
    # Verify dry run message was printed
    assert any("Dry run mode" in str(call) for call in mock_print.call_args_list)


def test_030():
    """
    Path traversal rejected | Auto | Issue number "../etc/passwd" |
    ValueError raised | Clear error message, no file access
    """
    # TDD: Arrange
    malicious_ids = [
        "../etc/passwd",
        "../../secrets",
        "..\\windows\\system32",
        "/etc/passwd",
        "C:\\Windows\\System32",
    ]
    
    # TDD: Act & Assert
    for mal_id in malicious_ids:
        with pytest.raises(ValueError) as exc_info:
            sanitize_identifier(mal_id)
        assert "Invalid identifier" in str(exc_info.value)


def test_040(mock_credentials):
    """
    Credential exhaustion pauses workers | Auto | 5 items, 2 credentials,
    --parallel 5 | Workers pause, resume on release | Log shows
    "[COORDINATOR] Credential pool exhausted"
    """
    # TDD: Arrange
    items = [f"item-{i:03d}" for i in range(5)]
    cred_coordinator = CredentialCoordinator(["key1", "key2"])
    
    def worker_func(item, credential):
        time.sleep(0.1)  # Simulate work
        
    coordinator = ParallelCoordinator(
        max_workers=5,
        credential_coordinator=cred_coordinator,
    )
    
    # TDD: Act
    with patch('builtins.print') as mock_print:
        stats, results = coordinator.execute_parallel(
            items=items,
            worker_func=worker_func,
            item_id_func=lambda x: x,
            dry_run=False,
        )
    
    # TDD: Assert
    assert stats.total == 5
    assert stats.completed == 5
    # Check that exhaustion message was printed
    printed_output = "\n".join(str(call) for call in mock_print.call_args_list)
    assert "[COORDINATOR] Credential pool exhausted" in printed_output


def test_050():
    """
    HTTP 429 triggers backoff | Auto | AGENTOS_SIMULATE_429=true | Key
    marked rate-limited | Backoff applied, different key used or wait
    """
    # TDD: Arrange
    items = ["item-001"]
    cred_coordinator = CredentialCoordinator(["key1", "key2"])
    
    call_count = []
    
    def worker_func(item, credential):
        call_count.append(credential)
        
    coordinator = ParallelCoordinator(
        max_workers=1,
        credential_coordinator=cred_coordinator,
    )
    
    # TDD: Act
    with patch.dict(os.environ, {"AGENTOS_SIMULATE_429": "true"}):
        stats, results = coordinator.execute_parallel(
            items=items,
            worker_func=worker_func,
            item_id_func=lambda x: x,
            dry_run=False,
        )
    
    # TDD: Assert
    assert stats.total == 1
    assert stats.completed == 1
    # Should have gotten a different credential after rate limit
    assert call_count[0] is not None


def test_060():
    """
    Single workflow failure isolated | Auto | 1 invalid spec among 3 | 2
    succeed, 1 fails | Failed item in report, others complete
    """
    # TDD: Arrange
    items = ["good-001", "bad-002", "good-003"]
    
    def worker_func(item, credential):
        if "bad" in item:
            raise ValueError(f"Invalid item: {item}")
            
    coordinator = ParallelCoordinator(max_workers=3)
    
    # TDD: Act
    stats, results = coordinator.execute_parallel(
        items=items,
        worker_func=worker_func,
        item_id_func=lambda x: x,
        dry_run=False,
    )
    
    # TDD: Assert
    assert stats.total == 3
    assert stats.completed == 3
    assert stats.failed == 1
    assert stats.success_count == 2
    
    # Check specific results
    failed_results = [r for r in results if not r.success]
    assert len(failed_results) == 1
    assert "bad" in failed_results[0].item_id


def test_070():
    """
    Graceful shutdown on SIGINT | Auto | SIGINT during execution |
    Workers checkpoint and exit | All checkpoint DBs written within 5s
    """
    # TDD: Arrange
    items = [f"item-{i:03d}" for i in range(5)]
    
    def worker_func(item, credential):
        time.sleep(0.5)  # Simulate work
        
    coordinator = ParallelCoordinator(max_workers=2)
    
    # Function to send SIGINT after a delay
    def send_interrupt():
        time.sleep(0.2)
        coordinator._handle_shutdown_signal(signal.SIGINT, None)
        
    # TDD: Act
    interrupt_thread = threading.Thread(target=send_interrupt)
    interrupt_thread.start()
    
    start_time = time.time()
    stats, results = coordinator.execute_parallel(
        items=items,
        worker_func=worker_func,
        item_id_func=lambda x: x,
        dry_run=False,
    )
    duration = time.time() - start_time
    
    interrupt_thread.join()
    
    # TDD: Assert
    assert duration < 5.0, "Shutdown took too long"
    checkpoints = coordinator.get_checkpoints()
    # Some items should have been interrupted
    assert len(checkpoints) > 0


def test_080():
    """
    Output prefix prevents interleaving | Auto | 3 parallel workflows |
    All lines prefixed correctly | No partial line mixing
    """
    # TDD: Arrange
    prefixes = ["[LLD-001]", "[LLD-002]", "[LLD-003]"]
    output_lines = []
    
    def capture_output(text):
        output_lines.append(text)
        
    mock_stream = Mock()
    mock_stream.write = capture_output
    mock_stream.flush = Mock()
    
    # TDD: Act
    for prefix in prefixes:
        prefixer = OutputPrefixer(prefix, stream=mock_stream)
        prefixer.write("Line 1\n")
        prefixer.write("Line 2\n")
        prefixer.flush()
    
    # TDD: Assert
    # Check all lines have prefixes
    for line in output_lines:
        if line.strip():  # Skip empty lines
            assert any(prefix in line for prefix in prefixes)
            
    # Check no partial line mixing (each line should be complete)
    for line in output_lines:
        if "[LLD-" in line:
            # Should have complete format: "[LLD-XXX] content\n"
            assert line.count("[LLD-") == 1


def test_090():
    """
    Performance benchmark | Auto-Live | 6 items, sequential vs --parallel
    3 | Parallel < 50% sequential time | Timing comparison logged
    """
    # TDD: Arrange
    items = [f"item-{i:03d}" for i in range(6)]
    work_duration = 0.1
    
    def worker_func(item, credential):
        time.sleep(work_duration)
        
    # TDD: Act - Sequential
    sequential_start = time.time()
    for item in items:
        worker_func(item, None)
    sequential_duration = time.time() - sequential_start
    
    # TDD: Act - Parallel
    coordinator = ParallelCoordinator(max_workers=3)
    parallel_start = time.time()
    stats, results = coordinator.execute_parallel(
        items=items,
        worker_func=worker_func,
        item_id_func=lambda x: x,
        dry_run=False,
    )
    parallel_duration = time.time() - parallel_start
    
    # TDD: Assert
    assert stats.completed == 6
    assert stats.success_count == 6
    # Parallel should be significantly faster (allow some overhead)
    assert parallel_duration < sequential_duration * 0.6, \
        f"Parallel ({parallel_duration:.2f}s) not faster than sequential ({sequential_duration:.2f}s)"


def test_100():
    """
    Max parallelism enforced | Auto | Capped to 10 | Warning logged, runs
    with 10
    """
    # TDD: Arrange
    excessive_parallelism = 20
    
    # TDD: Act
    with patch('builtins.print') as mock_print:
        coordinator = ParallelCoordinator(max_workers=excessive_parallelism)
    
    # TDD: Assert
    assert coordinator.max_workers == ParallelCoordinator.MAX_PARALLELISM
    # Check warning was logged
    printed_output = "\n".join(str(call) for call in mock_print.call_args_list)
    assert "Warning" in printed_output
    assert str(ParallelCoordinator.MAX_PARALLELISM) in printed_output


def test_110():
    """
    Default parallelism applied | Auto | Uses 3 | Config shows
    max_parallelism=3
    """
    # TDD: Arrange & Act
    coordinator = ParallelCoordinator()
    
    # TDD: Assert
    assert coordinator.max_workers == ParallelCoordinator.DEFAULT_PARALLELISM
    assert coordinator.max_workers == 3


# Additional coverage tests
# -------------------------

def test_120_credential_timeout():
    """
    Test credential acquisition timeout | Coverage | All credentials busy,
    acquire with timeout | Returns None after timeout
    """
    # TDD: Arrange
    cred_coordinator = CredentialCoordinator(["key1"])
    
    # Acquire the only credential
    key = cred_coordinator.acquire(timeout=0.1)
    assert key == "key1"
    
    # TDD: Act - Try to acquire again with short timeout
    result = cred_coordinator.acquire(timeout=0.1)
    
    # TDD: Assert
    assert result is None  # Should timeout


def test_130_credential_rate_limit_release():
    """
    Test releasing credential with rate limit | Coverage | Release with
    rate_limited=True | Prints backoff message
    """
    # TDD: Arrange
    cred_coordinator = CredentialCoordinator(["key1", "key2"])
    key = cred_coordinator.acquire()
    
    # TDD: Act
    with patch('builtins.print') as mock_print:
        cred_coordinator.release(key, rate_limited=True, backoff_seconds=30.0)
    
    # TDD: Assert
    printed_output = "\n".join(str(call) for call in mock_print.call_args_list)
    assert "rate-limited" in printed_output
    assert "30" in printed_output


def test_140_sanitize_path_separators():
    """
    Test path separator rejection | Coverage | Identifiers with / or \\ |
    ValueError raised
    """
    # TDD: Arrange
    invalid_ids = [
        "path/to/something",
        "path\\to\\something",
        "item-001/subitem",
    ]
    
    # TDD: Act & Assert
    for invalid_id in invalid_ids:
        with pytest.raises(ValueError) as exc_info:
            sanitize_identifier(invalid_id)
        assert "path separators not allowed" in str(exc_info.value)


def test_150_sanitize_invalid_characters():
    """
    Test invalid character rejection | Coverage | Identifiers with special
    chars | ValueError raised
    """
    # TDD: Arrange
    invalid_ids = [
        "item@001",
        "item#001",
        "item 001",  # space
        "item$001",
    ]
    
    # TDD: Act & Assert
    for invalid_id in invalid_ids:
        with pytest.raises(ValueError) as exc_info:
            sanitize_identifier(invalid_id)
        assert "invalid characters" in str(exc_info.value)


def test_160_output_prefixer_empty_lines():
    """
    Test output prefixer with empty lines | Coverage | Write empty line |
    No prefix added to empty line
    """
    # TDD: Arrange
    output_lines = []
    
    def capture_output(text):
        output_lines.append(text)
        
    mock_stream = Mock()
    mock_stream.write = capture_output
    mock_stream.flush = Mock()
    
    prefixer = OutputPrefixer("[TEST]", stream=mock_stream)
    
    # TDD: Act
    prefixer.write("Content\n")
    prefixer.write("\n")  # Empty line
    prefixer.write("More content\n")
    
    # TDD: Assert
    assert "[TEST] Content\n" in output_lines
    assert "\n" in output_lines  # Empty line without prefix
    assert "[TEST] More content\n" in output_lines


def test_170_output_prefixer_flush_buffer():
    """
    Test output prefixer flush with buffered content | Coverage | Write
    without newline, then flush | Buffered content output with prefix
    """
    # TDD: Arrange
    output_lines = []
    
    def capture_output(text):
        output_lines.append(text)
        
    mock_stream = Mock()
    mock_stream.write = capture_output
    mock_stream.flush = Mock()
    
    prefixer = OutputPrefixer("[TEST]", stream=mock_stream)
    
    # TDD: Act
    prefixer.write("Incomplete line without newline")
    prefixer.flush()
    
    # TDD: Assert
    assert any("[TEST] Incomplete line without newline" in line for line in output_lines)


def test_180_coordinator_credential_acquisition_failure():
    """
    Test coordinator handling credential acquisition failure | Coverage |
    Credential acquire returns None | RuntimeError raised in worker
    """
    # TDD: Arrange
    items = ["item-001"]
    
    # Create mock credential coordinator that always returns None
    mock_cred_coordinator = Mock()
    mock_cred_coordinator.acquire = Mock(return_value=None)
    mock_cred_coordinator.release = Mock()
    
    def worker_func(item, credential):
        pass
        
    coordinator = ParallelCoordinator(
        max_workers=1,
        credential_coordinator=mock_cred_coordinator,
    )
    
    # TDD: Act
    stats, results = coordinator.execute_parallel(
        items=items,
        worker_func=worker_func,
        item_id_func=lambda x: x,
        dry_run=False,
    )
    
    # TDD: Assert
    assert stats.total == 1
    assert stats.completed == 1
    assert stats.failed == 1
    assert len(results) == 1
    assert not results[0].success
    assert "Failed to acquire credential" in results[0].error
```

### Previous Test Run (FAILED)

The previous implementation attempt failed. Here's the test output:

```
sting\__init__.py                        3      3     0%   18-21
agentos\workflows\testing\audit.py                          82     82     0%   13-307
agentos\workflows\testing\graph.py                          98     98     0%   41-363
agentos\workflows\testing\knowledge\__init__.py              2      2     0%   8-10
agentos\workflows\testing\knowledge\patterns.py             60     60     0%   7-193
agentos\workflows\testing\nodes\__init__.py                  9      9     0%   19-31
agentos\workflows\testing\nodes\document.py                140    140     0%   13-360
agentos\workflows\testing\nodes\e2e_validation.py           84     84     0%   9-280
agentos\workflows\testing\nodes\finalize.py                 46     46     0%   9-140
agentos\workflows\testing\nodes\implement_code.py          190    190     0%   7-582
agentos\workflows\testing\nodes\load_lld.py                192    192     0%   10-548
agentos\workflows\testing\nodes\review_test_plan.py        139    139     0%   9-496
agentos\workflows\testing\nodes\scaffold_tests.py          157    157     0%   9-372
agentos\workflows\testing\nodes\verify_phases.py           139    139     0%   7-455
agentos\workflows\testing\state.py                           9      9     0%   12-36
agentos\workflows\testing\templates\__init__.py              5      5     0%   12-23
agentos\workflows\testing\templates\cp_docs.py              73     73     0%   9-296
agentos\workflows\testing\templates\lessons.py             115    115     0%   8-304
agentos\workflows\testing\templates\runbook.py              94     94     0%   8-259
agentos\workflows\testing\templates\wiki_page.py            74     74     0%   8-207
--------------------------------------------------------------------------------------
TOTAL                                                     5329   5148     3%

FAIL Required test coverage of 95% not reached. Total coverage: 3.40%

============================= 18 passed in 2.17s ==============================


```

Please fix the issues and provide updated implementation.

## Instructions

1. Generate implementation code that makes all tests pass
2. Follow the patterns established in the codebase
3. Ensure proper error handling
4. Add type hints where appropriate
5. Keep the implementation minimal - only what's needed to pass tests

## Output Format (CRITICAL - MUST FOLLOW EXACTLY)

For EACH file you need to create or modify, provide a code block with this EXACT format:

```python
# File: path/to/implementation.py

def function_name():
    ...
```

**Rules:**
- The `# File: path/to/file` comment MUST be the FIRST line inside the code block
- Use the language-appropriate code fence (```python, ```gitignore, ```yaml, etc.)
- Path must be relative to repository root (e.g., `src/module/file.py`)
- Do NOT include "(append)" or other annotations in the path
- Provide complete file contents, not patches or diffs

**Example for .gitignore:**
```gitignore
# File: .gitignore

# Existing patterns...
*.pyc
__pycache__/

# New pattern
.agentos/
```

If multiple files are needed, provide each in a separate code block with its own `# File:` header.
