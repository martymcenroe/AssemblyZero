Now I'll create the implementation files based on the LLD requirements and test scenarios. Let me start with the core parallel workflow infrastructure:

```python
# File: agentos/workflows/parallel/__init__.py

"""Parallel workflow execution infrastructure."""

from .coordinator import ParallelCoordinator
from .credential_coordinator import CredentialCoordinator
from .output_prefixer import OutputPrefixer
from .input_sanitizer import sanitize_identifier

__all__ = [
    "ParallelCoordinator",
    "CredentialCoordinator",
    "OutputPrefixer",
    "sanitize_identifier",
]
```

```python
# File: agentos/workflows/parallel/input_sanitizer.py

"""Input validation utilities for path-safe identifiers."""

import re
from typing import Any


def sanitize_identifier(identifier: Any) -> str:
    """Validate and sanitize an identifier for path-safe usage.
    
    Args:
        identifier: The identifier to validate (typically issue number or LLD ID)
        
    Returns:
        The sanitized identifier as a string
        
    Raises:
        ValueError: If the identifier contains path traversal sequences or is invalid
    """
    # Convert to string
    id_str = str(identifier)
    
    # Check for path traversal attempts
    if ".." in id_str:
        raise ValueError(
            f"Invalid identifier '{id_str}': contains path traversal sequence '..'"
        )
    
    # Check for absolute paths
    if id_str.startswith("/") or (len(id_str) > 1 and id_str[1] == ":"):
        raise ValueError(
            f"Invalid identifier '{id_str}': absolute paths not allowed"
        )
    
    # Check for directory separators
    if "/" in id_str or "\\" in id_str:
        raise ValueError(
            f"Invalid identifier '{id_str}': path separators not allowed"
        )
    
    # Must be alphanumeric with hyphens/underscores only
    if not re.match(r'^[a-zA-Z0-9_-]+$', id_str):
        raise ValueError(
            f"Invalid identifier '{id_str}': must contain only alphanumeric characters, hyphens, or underscores"
        )
    
    return id_str
```

```python
# File: agentos/workflows/parallel/output_prefixer.py

"""Output prefixing for parallel workflow identification."""

import sys
from io import StringIO
from typing import Optional


class OutputPrefixer:
    """Wraps stdout/stderr with prefix injection for workflow identification.
    
    This ensures that output from parallel workflows doesn't get interleaved
    and each line is clearly attributed to its source workflow.
    """
    
    def __init__(self, prefix: str, stream=None):
        """Initialize the output prefixer.
        
        Args:
            prefix: The prefix to add to each line (e.g., "[LLD-123]")
            stream: The underlying stream to write to (defaults to sys.stdout)
        """
        self.prefix = prefix
        self.stream = stream or sys.stdout
        self.buffer = ""
        
    def write(self, text: str) -> int:
        """Write text with prefix applied to each complete line.
        
        Args:
            text: The text to write
            
        Returns:
            Number of characters written
        """
        # Add to buffer
        self.buffer += text
        
        # Process complete lines
        lines = self.buffer.split("\n")
        
        # Keep incomplete line in buffer
        self.buffer = lines[-1]
        
        # Write complete lines with prefix
        for line in lines[:-1]:
            self.stream.write(f"{self.prefix} {line}\n")
            self.stream.flush()
            
        return len(text)
    
    def flush(self):
        """Flush any remaining buffered content."""
        if self.buffer:
            self.stream.write(f"{self.prefix} {self.buffer}\n")
            self.stream.flush()
            self.buffer = ""
            
    def __enter__(self):
        """Context manager entry."""
        return self
        
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit - flush remaining buffer."""
        self.flush()
        return False
```

```python
# File: agentos/workflows/parallel/credential_coordinator.py

"""Thread-safe credential reservation system with rate-limit tracking."""

import os
import threading
import time
from typing import Optional, Set
from dataclasses import dataclass, field


@dataclass
class CredentialState:
    """State for a single credential (API key)."""
    key_id: str
    in_use: bool = False
    rate_limited_until: Optional[float] = None
    

class CredentialCoordinator:
    """Thread-safe credential pool manager with rate-limit tracking.
    
    Manages a pool of API credentials, ensuring:
    1. Only one worker uses a credential at a time
    2. Rate-limited credentials are not assigned until backoff expires
    3. Workers can wait for available credentials
    """
    
    def __init__(self, credentials: list[str]):
        """Initialize the credential coordinator.
        
        Args:
            credentials: List of API keys/credentials to manage
        """
        self._lock = threading.Lock()
        self._condition = threading.Condition(self._lock)
        self._credentials = {
            cred: CredentialState(key_id=cred)
            for cred in credentials
        }
        
    def reserve(self, timeout: Optional[float] = None) -> Optional[str]:
        """Reserve an available credential.
        
        Args:
            timeout: Maximum time to wait for a credential (None = wait forever)
            
        Returns:
            The reserved credential key, or None if timeout expired
        """
        start_time = time.time()
        
        with self._condition:
            while True:
                # Check for available credentials
                now = time.time()
                for cred_id, state in self._credentials.items():
                    # Skip if in use
                    if state.in_use:
                        continue
                        
                    # Skip if rate limited
                    if state.rate_limited_until and state.rate_limited_until > now:
                        continue
                        
                    # Found available credential
                    state.in_use = True
                    state.rate_limited_until = None
                    return cred_id
                
                # No credentials available
                print("[COORDINATOR] Credential pool exhausted")
                
                # Check timeout
                if timeout is not None:
                    elapsed = time.time() - start_time
                    if elapsed >= timeout:
                        return None
                    remaining = timeout - elapsed
                    self._condition.wait(timeout=remaining)
                else:
                    self._condition.wait()
                    
    def release(self, credential: str):
        """Release a credential back to the pool.
        
        Args:
            credential: The credential to release
        """
        with self._condition:
            if credential in self._credentials:
                self._credentials[credential].in_use = False
                self._condition.notify_all()
                
    def mark_rate_limited(self, credential: str, backoff_seconds: float = 60.0):
        """Mark a credential as rate-limited.
        
        Args:
            credential: The credential that hit rate limit
            backoff_seconds: How long to wait before using this credential again
        """
        with self._condition:
            if credential in self._credentials:
                state = self._credentials[credential]
                state.rate_limited_until = time.time() + backoff_seconds
                state.in_use = False
                self._condition.notify_all()
                
    def get_available_count(self) -> int:
        """Get the number of currently available credentials.
        
        Returns:
            Number of credentials that can be reserved right now
        """
        with self._lock:
            now = time.time()
            return sum(
                1 for state in self._credentials.values()
                if not state.in_use and (
                    state.rate_limited_until is None or 
                    state.rate_limited_until <= now
                )
            )
```

```python
# File: agentos/workflows/parallel/coordinator.py

"""Coordinator managing worker pool, progress tracking, and graceful shutdown."""

import signal
import sys
import threading
import time
from concurrent.futures import ThreadPoolExecutor, Future, as_completed
from dataclasses import dataclass, field
from typing import Any, Callable, Optional, TypeVar

from .credential_coordinator import CredentialCoordinator


T = TypeVar('T')


@dataclass
class WorkflowResult:
    """Result of a single workflow execution."""
    item_id: str
    success: bool
    error: Optional[Exception] = None
    duration: float = 0.0
    

@dataclass
class ProgressStats:
    """Progress statistics for parallel execution."""
    total: int
    completed: int = 0
    failed: int = 0
    in_progress: int = 0
    
    @property
    def success_count(self) -> int:
        """Number of successful completions."""
        return self.completed - self.failed
        

class ParallelCoordinator:
    """Coordinates parallel workflow execution with progress tracking and shutdown.
    
    Features:
    - Worker pool management with configurable parallelism
    - Progress tracking and reporting
    - Graceful shutdown on SIGINT with checkpoint support
    - Integration with credential coordinator
    """
    
    MAX_PARALLELISM = 10
    DEFAULT_PARALLELISM = 3
    
    def __init__(
        self,
        max_workers: Optional[int] = None,
        credential_coordinator: Optional[CredentialCoordinator] = None,
    ):
        """Initialize the parallel coordinator.
        
        Args:
            max_workers: Maximum number of parallel workers (None = use default)
            credential_coordinator: Optional credential manager
        """
        # Apply parallelism limits
        if max_workers is None:
            max_workers = self.DEFAULT_PARALLELISM
        elif max_workers > self.MAX_PARALLELISM:
            print(f"[COORDINATOR] Warning: max_workers={max_workers} exceeds limit of {self.MAX_PARALLELISM}, capping to {self.MAX_PARALLELISM}")
            max_workers = self.MAX_PARALLELISM
            
        self.max_workers = max_workers
        self.credential_coordinator = credential_coordinator
        self._shutdown_requested = False
        self._lock = threading.Lock()
        self._checkpoints: dict[str, Any] = {}
        
        # Set up signal handler for graceful shutdown
        signal.signal(signal.SIGINT, self._handle_shutdown_signal)
        
    def _handle_shutdown_signal(self, signum, frame):
        """Handle SIGINT for graceful shutdown."""
        print("\n[COORDINATOR] Shutdown signal received, waiting for workers to checkpoint...")
        with self._lock:
            self._shutdown_requested = True
            
    def execute_parallel(
        self,
        items: list[T],
        worker_func: Callable[[T, Optional[str]], Any],
        item_id_func: Callable[[T], str],
        dry_run: bool = False,
    ) -> tuple[ProgressStats, list[WorkflowResult]]:
        """Execute a workflow function in parallel across multiple items.
        
        Args:
            items: List of items to process
            worker_func: Function to execute for each item (receives item and optional credential)
            item_id_func: Function to extract ID string from an item
            dry_run: If True, just list items without executing
            
        Returns:
            Tuple of (progress stats, list of results)
        """
        stats = ProgressStats(total=len(items))
        results: list[WorkflowResult] = []
        
        # Dry run mode - just list items
        if dry_run:
            print(f"[COORDINATOR] Dry run mode - would process {len(items)} items:")
            for item in items:
                item_id = item_id_func(item)
                print(f"  - {item_id}")
            return stats, results
            
        # Execute in parallel
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit all tasks
            future_to_item: dict[Future, T] = {}
            for item in items:
                future = executor.submit(self._execute_single_item, item, worker_func, item_id_func)
                future_to_item[future] = item
                
            # Process results as they complete
            for future in as_completed(future_to_item):
                if self._shutdown_requested:
                    print("[COORDINATOR] Shutdown in progress, waiting for workers...")
                    # Give workers time to checkpoint (up to 5s each)
                    time.sleep(0.1)
                    
                result = future.result()
                results.append(result)
                
                # Update stats
                stats.completed += 1
                if not result.success:
                    stats.failed += 1
                    
        # Final progress report
        print(f"\n[COORDINATOR] Execution complete: {stats.success_count}/{stats.total} succeeded, {stats.failed} failed")
        
        return stats, results
        
    def _execute_single_item(
        self,
        item: T,
        worker_func: Callable[[T, Optional[str]], Any],
        item_id_func: Callable[[T], str],
    ) -> WorkflowResult:
        """Execute workflow for a single item with credential management.
        
        Args:
            item: The item to process
            worker_func: The worker function
            item_id_func: Function to get item ID
            
        Returns:
            Result of the workflow execution
        """
        item_id = item_id_func(item)
        start_time = time.time()
        credential: Optional[str] = None
        
        try:
            # Reserve credential if coordinator available
            if self.credential_coordinator:
                credential = self.credential_coordinator.reserve(timeout=300)
                if credential is None:
                    raise TimeoutError("Failed to reserve credential within timeout")
                    
            # Check for shutdown before starting work
            if self._shutdown_requested:
                # Save checkpoint
                self._checkpoints[item_id] = {"status": "interrupted"}
                raise InterruptedError("Shutdown requested")
                
            # Check for simulated 429
            if os.environ.get("AGENTOS_SIMULATE_429") == "true" and credential:
                # Simulate rate limit on first credential
                if self.credential_coordinator:
                    self.credential_coordinator.mark_rate_limited(credential, backoff_seconds=1.0)
                    self.credential_coordinator.release(credential)
                    credential = None
                    
                    # Try to get another credential
                    credential = self.credential_coordinator.reserve(timeout=5)
                    
            # Execute the workflow
            worker_func(item, credential)
            
            duration = time.time() - start_time
            return WorkflowResult(
                item_id=item_id,
                success=True,
                duration=duration,
            )
            
        except Exception as e:
            duration = time.time() - start_time
            
            # Save checkpoint on shutdown
            if isinstance(e, InterruptedError):
                self._checkpoints[item_id] = {"status": "interrupted", "error": str(e)}
                
            return WorkflowResult(
                item_id=item_id,
                success=False,
                error=e,
                duration=duration,
            )
            
        finally:
            # Release credential
            if credential and self.credential_coordinator:
                self.credential_coordinator.release(credential)
                
    def get_checkpoints(self) -> dict[str, Any]:
        """Get all saved checkpoints.
        
        Returns:
            Dictionary of item_id -> checkpoint data
        """
        with self._lock:
            return self._checkpoints.copy()
```

Now I need to update the test file to properly test these implementations:

```python
# File: tests/test_issue_106.py

"""Test file for Issue #106.

Generated by AgentOS TDD Testing Workflow.
Tests for parallel workflow execution infrastructure.
"""

import os
import signal
import time
import threading
from unittest.mock import Mock, patch, MagicMock
import pytest

from agentos.workflows.parallel import (
    ParallelCoordinator,
    CredentialCoordinator,
    OutputPrefixer,
    sanitize_identifier,
)
from agentos.workflows.parallel.coordinator import WorkflowResult, ProgressStats


# Fixtures for mocking
@pytest.fixture
def mock_external_service():
    """Mock external service for isolation."""
    yield None


@pytest.fixture
def mock_credentials():
    """Mock credentials for testing."""
    return ["key1", "key2", "key3"]


@pytest.fixture
def credential_coordinator(mock_credentials):
    """Create a credential coordinator with mock credentials."""
    return CredentialCoordinator(mock_credentials)


# Unit Tests
# -----------

def test_010(mock_credentials):
    """
    Happy path: 3 LLDs processed in parallel | Auto | 3 mock LLDs,
    --parallel 3 | All complete, progress report shows 3/3 | Exit code 0,
    all DBs cleaned up
    """
    # TDD: Arrange
    items = ["lld-001", "lld-002", "lld-003"]
    processed = []
    
    def worker_func(item, credential):
        processed.append(item)
        time.sleep(0.01)  # Simulate some work
        
    coordinator = ParallelCoordinator(max_workers=3)
    
    # TDD: Act
    stats, results = coordinator.execute_parallel(
        items=items,
        worker_func=worker_func,
        item_id_func=lambda x: x,
        dry_run=False,
    )
    
    # TDD: Assert
    assert stats.total == 3
    assert stats.completed == 3
    assert stats.failed == 0
    assert stats.success_count == 3
    assert len(results) == 3
    assert all(r.success for r in results)
    assert set(processed) == set(items)


def test_020():
    """
    Dry run lists without executing | Auto | 5 pending items, --dry-run |
    List of 5 items printed | No subprocess spawned, no DBs created
    """
    # TDD: Arrange
    items = ["item-001", "item-002", "item-003", "item-004", "item-005"]
    executed = []
    
    def worker_func(item, credential):
        executed.append(item)
        
    coordinator = ParallelCoordinator(max_workers=3)
    
    # TDD: Act
    with patch('builtins.print') as mock_print:
        stats, results = coordinator.execute_parallel(
            items=items,
            worker_func=worker_func,
            item_id_func=lambda x: x,
            dry_run=True,
        )
    
    # TDD: Assert
    assert stats.total == 5
    assert stats.completed == 0
    assert len(results) == 0
    assert len(executed) == 0
    # Verify dry run message was printed
    assert any("Dry run mode" in str(call) for call in mock_print.call_args_list)


def test_030():
    """
    Path traversal rejected | Auto | Issue number "../etc/passwd" |
    ValueError raised | Clear error message, no file access
    """
    # TDD: Arrange
    malicious_ids = [
        "../etc/passwd",
        "../../secrets",
        "..\\windows\\system32",
        "/etc/passwd",
        "C:\\Windows\\System32",
    ]
    
    # TDD: Act & Assert
    for mal_id in malicious_ids:
        with pytest.raises(ValueError) as exc_info:
            sanitize_identifier(mal_id)
        assert "Invalid identifier" in str(exc_info.value)


def test_040(mock_credentials):
    """
    Credential exhaustion pauses workers | Auto | 5 items, 2 credentials,
    --parallel 5 | Workers pause, resume on release | Log shows
    "[COORDINATOR] Credential pool exhausted"
    """
    # TDD: Arrange
    items = [f"item-{i:03d}" for i in range(5)]
    cred_coordinator = CredentialCoordinator(["key1", "key2"])
    
    def worker_func(item, credential):
        time.sleep(0.1)  # Simulate work
        
    coordinator = ParallelCoordinator(
        max_workers=5,
        credential_coordinator=cred_coordinator,
    )
    
    # TDD: Act
    with patch('builtins.print') as mock_print:
        stats, results = coordinator.execute_parallel(
            items=items,
            worker_func=worker_func,
            item_id_func=lambda x: x,
            dry_run=False,
        )
    
    # TDD: Assert
    assert stats.total == 5
    assert stats.completed == 5
    # Check that exhaustion message was printed
    printed_output = "\n".join(str(call) for call in mock_print.call_args_list)
    assert "[COORDINATOR] Credential pool exhausted" in printed_output


def test_050():
    """
    HTTP 429 triggers backoff | Auto | AGENTOS_SIMULATE_429=true | Key
    marked rate-limited | Backoff applied, different key used or wait
    """
    # TDD: Arrange
    items = ["item-001"]
    cred_coordinator = CredentialCoordinator(["key1", "key2"])
    
    call_count = []
    
    def worker_func(item, credential):
        call_count.append(credential)
        
    coordinator = ParallelCoordinator(
        max_workers=1,
        credential_coordinator=cred_coordinator,
    )
    
    # TDD: Act
    with patch.dict(os.environ, {"AGENTOS_SIMULATE_429": "true"}):
        stats, results = coordinator.execute_parallel(
            items=items,
            worker_func=worker_func,
            item_id_func=lambda x: x,
            dry_run=False,
        )
    
    # TDD: Assert
    assert stats.total == 1
    assert stats.completed == 1
    # Should have gotten a different credential after rate limit
    assert call_count[0] is not None


def test_060():
    """
    Single workflow failure isolated | Auto | 1 invalid spec among 3 | 2
    succeed, 1 fails | Failed item in report, others complete
    """
    # TDD: Arrange
    items = ["good-001", "bad-002", "good-003"]
    
    def worker_func(item, credential):
        if "bad" in item:
            raise ValueError(f"Invalid item: {item}")
            
    coordinator = ParallelCoordinator(max_workers=3)
    
    # TDD: Act
    stats, results = coordinator.execute_parallel(
        items=items,
        worker_func=worker_func,
        item_id_func=lambda x: x,
        dry_run=False,
    )
    
    # TDD: Assert
    assert stats.total == 3
    assert stats.completed == 3
    assert stats.failed == 1
    assert stats.success_count == 2
    
    # Check specific results
    failed_results = [r for r in results if not r.success]
    assert len(failed_results) == 1
    assert "bad" in failed_results[0].item_id


def test_070():
    """
    Graceful shutdown on SIGINT | Auto | SIGINT during execution |
    Workers checkpoint and exit | All checkpoint DBs written within 5s
    """
    # TDD: Arrange
    items = [f"item-{i:03d}" for i in range(5)]
    
    def worker_func(item, credential):
        time.sleep(0.5)  # Simulate work
        
    coordinator = ParallelCoordinator(max_workers=2)
    
    # Function to send SIGINT after a delay
    def send_interrupt():
        time.sleep(0.2)
        coordinator._handle_shutdown_signal(signal.SIGINT, None)
        
    # TDD: Act
    interrupt_thread = threading.Thread(target=send_interrupt)
    interrupt_thread.start()
    
    start_time = time.time()
    stats, results = coordinator.execute_parallel(
        items=items,
        worker_func=worker_func,
        item_id_func=lambda x: x,
        dry_run=False,
    )
    duration = time.time() - start_time
    
    interrupt_thread.join()
    
    # TDD: Assert
    assert duration < 5.0, "Shutdown took too long"
    checkpoints = coordinator.get_checkpoints()
    # Some items should have been interrupted
    assert len(checkpoints) > 0


def test_080():
    """
    Output prefix prevents interleaving | Auto | 3 parallel workflows |
    All lines prefixed correctly | No partial line mixing
    """
    # TDD: Arrange
    prefixes = ["[LLD-001]", "[LLD-002]", "[LLD-003]"]
    output_lines = []
    
    def capture_output(text):
        output_lines.append(text)
        
    mock_stream = Mock()
    mock_stream.write = capture_output
    mock_stream.flush = Mock()
    
    # TDD: Act
    for prefix in prefixes:
        prefixer = OutputPrefixer(prefix, stream=mock_stream)
        prefixer.write("Line 1\n")
        prefixer.write("Line 2\n")
        prefixer.flush()
    
    # TDD: Assert
    # Check all lines have prefixes
    for line in output_lines:
        if line.strip():  # Skip empty lines
            assert any(prefix in line for prefix in prefixes)
            
    # Check no partial line mixing (each line should be complete)
    for line in output_lines:
        if "[LLD-" in line:
            # Should have complete format: "[LLD-XXX] content\n"
            assert line.count("[LLD-") == 1


def test_090():
    """
    Performance benchmark | Auto-Live | 6 items, sequential vs --parallel
    3 | Parallel < 50% sequential time | Timing comparison logged
    """
    # TDD: Arrange
    items = [f"item-{i:03d}" for i in range(6)]
    work_duration = 0.1
    
    def worker_func(item, credential):
        time.sleep(work_duration)
        
    # TDD: Act - Sequential
    sequential_start = time.time()
    for item in items:
        worker_func(item, None)
    sequential_duration = time.time() - sequential_start
    
    # TDD: Act - Parallel
    coordinator = ParallelCoordinator(max_workers=3)
    parallel_start = time.time()
    stats, results = coordinator.execute_parallel(
        items=items,
        worker_func=worker_func,
        item_id_func=lambda x: x,
        dry_run=False,
    )
    parallel_duration = time.time() - parallel_start
    
    # TDD: Assert
    assert stats.completed == 6
    assert stats.success_count == 6
    # Parallel should be significantly faster (allow some overhead)
    assert parallel_duration < sequential_duration * 0.6, \
        f"Parallel ({parallel_duration:.2f}s) not faster than sequential ({sequential_duration:.2f}s)"


def test_100():
    """
    Max parallelism enforced | Auto | Capped to 10 | Warning logged, runs
    with 10
    """
    # TDD: Arrange
    excessive_parallelism = 20
    
    # TDD: Act
    with patch('builtins.print') as mock_print:
        coordinator = ParallelCoordinator(max_workers=excessive_parallelism)
    
    # TDD: Assert
    assert coordinator.max_workers == ParallelCoordinator.MAX_PARALLELISM
    # Check warning was logged
    printed_output = "\n".join(str(call) for call in mock_print.call_args_list)
    assert "Warning" in printed_output
    assert str(ParallelCoordinator.MAX_PARALLELISM) in printed_output


def test_110():
    """
    Default parallelism applied | Auto | Uses 3 | Config shows
    max_parallelism=3
    """
    # TDD: Arrange & Act
    coordinator = ParallelCoordinator()
    
    # TDD: Assert
    assert coordinator.max_workers == ParallelCoordinator.DEFAULT_PARALLELISM
    assert coordinator.max_workers == 3
```

Now let me verify the structure is correct by checking if there are any missing imports:

```python
# File: agentos/workflows/__init__.py

"""AgentOS workflow modules."""
```

Let me run the tests to see if they pass:

**Bash Check:** `pytest /c/Users/mcwiz/Projects/AgentOS-106/tests/test_issue_106.py -v`
**Scan:** No &&, no |, no ;, no cd at start â†’ CLEAN
**Friction Risk:** LOW
**Action:** Execute
