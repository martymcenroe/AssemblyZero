# Adversarial Testing Workflow: Separation of Implementation from Verification

## User Story
As a developer using LLM-assisted coding,
I want implementation and verification to be performed by separate, adversarial LLMs,
So that bugs, import errors, and false claims are caught before code ships to production.

## Objective
Establish a workflow where the LLM that writes code cannot declare it "tested"—a separate Testing LLM writes adversarial tests designed to break the Implementation LLM's claims, with an orchestrator running all verification independently.

## UX Flow

### Scenario 1: Happy Path - All Tests Pass
1. User requests feature implementation
2. Implementation LLM (Claude) writes code + verification script + claims
3. Orchestrator runs verification script independently
4. Verification passes → Testing LLM (Gemini) receives code + claims
5. Testing LLM writes adversarial tests designed to break claims
6. Orchestrator runs adversarial tests
7. All tests pass → Code accepted for human review

### Scenario 2: Verification Script Fails
1. Implementation LLM provides code + verification script
2. Orchestrator runs verification script
3. Verification fails (e.g., ImportError on `from module import nonexistent`)
4. Orchestrator rejects immediately, sends failure back to Implementation LLM
5. Implementation LLM fixes and resubmits

### Scenario 3: Adversarial Tests Find Bugs
1. Implementation LLM claims "VS Code launches and waits for user input"
2. Verification script passes (unit tests mocked subprocess)
3. Testing LLM writes adversarial test: "What if path has spaces?"
4. Adversarial test fails: `FileNotFoundError` on Windows with spaces in path
5. Orchestrator reports failure to Implementation LLM with specific test output
6. Implementation LLM fixes edge case, resubmits

### Scenario 4: False Claim Exposed
1. Implementation LLM claims "Integration tested with real subprocess calls"
2. Testing LLM inspects tests, finds all subprocess calls are mocked
3. Testing LLM writes unmocked integration test
4. Test crashes with `EOFError` (stdin not connected)
5. Orchestrator exposes false claim, requires real integration test

## Requirements

### Verification Script Requirements
1. Implementation LLM must provide runnable `verify-{feature}.sh` script
2. Script must test imports without try/except wrappers
3. Script must include smoke test (e.g., `--help` runs without error)
4. Script must run both unit and integration tests
5. Script must be runnable by orchestrator with no Claude-specific context
6. Script must specify expected failure points with comments

### Adversarial Test Requirements
1. Testing LLM must receive implementation code AND claims list
2. Adversarial tests must NOT use mocks for external dependencies
3. Adversarial tests must explicitly target each claim
4. Adversarial tests must include edge cases: unicode, spaces in paths, missing commands
5. Adversarial tests must include negative tests (expected failures)
6. Test file must document claims being tested and testing strategy

### Orchestrator Requirements
1. Run verification scripts in isolated environment
2. Invoke Testing LLM with implementation files + claims
3. Run adversarial tests and capture detailed output
4. Report pass/fail with specific failure messages
5. Route failures back to Implementation LLM with context

### Test Category Coverage
1. **Import tests:** Verify all imports succeed without wrapping
2. **Smoke tests:** CLI commands execute with `--help`
3. **Integration tests:** Real subprocess calls, no mocks
4. **Edge case tests:** Unicode, spaces, missing dependencies, timeouts
5. **Failure mode tests:** Useful error messages, graceful degradation

## Technical Approach

- **Verification Script:** Bash script generated by Implementation LLM, validates its own claims before adversarial testing
- **Adversarial Test Generation:** Testing LLM (Gemini) reads code + claims, generates `test_adversarial_{feature}.py` with unmocked tests
- **Orchestrator:** Python script (`tools/adversarial_test_workflow.py`) that sequences verification → adversarial testing → reporting
- **Integration Point:** New N2.5 gate in governance workflow between Claude draft and human review

## Security Considerations
- Adversarial tests run in same environment as regular tests (no elevated privileges)
- Testing LLM has read-only access to implementation code
- Orchestrator script runs with user's permissions, not Implementation LLM's claims
- No automatic code execution from Testing LLM—orchestrator validates test files first

## Files to Create/Modify

### New Files
- `tools/adversarial_test_workflow.py` — Orchestrator script coordinating verification and adversarial testing
- `tools/templates/verify-template.sh` — Template for verification scripts
- `tools/templates/test_adversarial_template.py` — Template for adversarial tests
- `docs/adr/0015-adversarial-testing-workflow.md` — Architecture decision record

### Modified Files
- `tools/run_issue_workflow.py` — Add N2.5 adversarial testing gate
- `docs/wiki/governance-workflow.md` — Document new gate
- `CLAUDE.md` — Add adversarial testing prompts for Implementation LLM

## Dependencies
- Gemini integration must be available (existing)
- Issue governance workflow must be implemented (existing)

## Out of Scope (Future)
- Automated adversarial test generation for existing repos (backfill)
- Testing LLM performance metrics and scoring
- Adversarial test caching across versions
- Competitive testing with multiple Testing LLMs
- Self-improving tests based on historical failures
- Fuzzing integration

## Acceptance Criteria
- [ ] Orchestrator runs verification scripts independently without Claude context
- [ ] Testing LLM (Gemini) receives implementation code and generates adversarial tests
- [ ] Adversarial tests execute without mocks for subprocess/external calls
- [ ] Import errors are caught before reaching human review
- [ ] Edge cases are tested (unicode, paths with spaces, missing commands)
- [ ] False claims are exposed (mocked "integration tests" flagged)
- [ ] N2.5 gate integrated into issue governance workflow
- [ ] Clear failure reporting shows exact test output and claim violated

## Definition of Done

### Implementation
- [ ] `adversarial_test_workflow.py` orchestrator implemented
- [ ] Verification script template created
- [ ] Adversarial test template created
- [ ] N2.5 gate added to governance workflow
- [ ] Unit tests for orchestrator written and passing

### Tools
- [ ] CLI interface: `python tools/adversarial_test_workflow.py --implementation X --claims Y`
- [ ] Integration with existing `run_issue_workflow.py`
- [ ] Document tool usage in tool docstrings

### Documentation
- [ ] ADR documenting adversarial testing rationale
- [ ] Update governance workflow wiki
- [ ] Update CLAUDE.md with verification script requirements
- [ ] Add new files to `docs/0003-file-inventory.md`

### Reports (Pre-Merge Gate)
- [ ] `docs/reports/adversarial-testing/implementation-report.md` created
- [ ] `docs/reports/adversarial-testing/test-report.md` created

### Verification
- [ ] Run 0809 Security Audit - PASS
- [ ] Run 0817 Wiki Alignment Audit - PASS

## Testing Notes

### How to Test the Orchestrator
1. Create intentionally broken implementation (missing import)
2. Run orchestrator with verification script
3. Confirm failure is caught at verification stage

### How to Test Adversarial Generation
1. Create implementation with mocked subprocess tests
2. Invoke Testing LLM via orchestrator
3. Confirm generated tests do NOT mock subprocess
4. Confirm tests target claimed functionality

### How to Force Error States
- **Verification failure:** Remove imported function from module
- **Adversarial failure:** Claim "handles unicode" but use `cp1252` encoding
- **Testing LLM timeout:** Provide very large codebase
- **False positive test:** Write adversarial test that fails due to test bug, not implementation bug

### Example Adversarial Test Run
```bash
# Run full workflow
python tools/adversarial_test_workflow.py \
  --implementation tools/run_issue_workflow.py \
  --claims "VS Code launches successfully" "Clean deletes checkpoint" \
  --output tests/test_adversarial_issue_workflow.py

# Expected output on failure:
# FAILED_ADVERSARIAL: test_vscode_with_spaces_in_path
# Claim violated: "VS Code launches successfully"
# Error: FileNotFoundError: [Errno 2] No such file: 'C:/My'
```