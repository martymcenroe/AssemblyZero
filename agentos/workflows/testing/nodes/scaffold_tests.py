"""N2: Scaffold Tests node for TDD Testing Workflow.

Generates executable test stubs from the approved test plan:
- Each test has `assert False, "TDD: Implementation pending"`
- Tests are syntactically valid and RUNNABLE
- Uses pytest conventions and fixtures
"""

import re
from pathlib import Path
from typing import Any

from agentos.workflows.testing.audit import (
    get_repo_root,
    log_workflow_execution,
    next_file_number,
    save_audit_file,
)
from agentos.workflows.testing.knowledge.patterns import get_test_type_info
from agentos.workflows.testing.state import TestingWorkflowState, TestScenario


def generate_test_file_content(
    scenarios: list[TestScenario],
    module_name: str,
    issue_number: int,
) -> str:
    """Generate pytest file content from test scenarios.

    Args:
        scenarios: List of test scenarios.
        module_name: Name of the module being tested.
        issue_number: GitHub issue number.

    Returns:
        Python test file content.
    """
    # Group scenarios by test type
    unit_tests = [s for s in scenarios if s.get("test_type") == "unit"]
    integration_tests = [s for s in scenarios if s.get("test_type") == "integration"]
    e2e_tests = [s for s in scenarios if s.get("test_type") == "e2e"]
    other_tests = [
        s for s in scenarios
        if s.get("test_type") not in ("unit", "integration", "e2e")
    ]

    lines = [
        '"""Test file for Issue #{issue_number}.',
        "",
        "Generated by AgentOS TDD Testing Workflow.",
        "Each test starts with `assert False` - implementation will make them pass.",
        '"""',
        "",
        "import pytest",
        "",
        "",
    ]

    # Add fixtures if needed
    if any(s.get("mock_needed") for s in scenarios):
        lines.extend([
            "# Fixtures for mocking",
            "@pytest.fixture",
            "def mock_external_service():",
            '    """Mock external service for isolation."""',
            "    # TODO: Implement mock",
            "    yield None",
            "",
            "",
        ])

    if integration_tests or e2e_tests:
        lines.extend([
            "# Integration/E2E fixtures",
            "@pytest.fixture",
            "def test_client():",
            '    """Test client for API calls."""',
            "    # TODO: Implement test client",
            "    yield None",
            "",
            "",
        ])

    # Generate unit tests
    if unit_tests:
        lines.append("# Unit Tests")
        lines.append("# -----------")
        lines.append("")

        for scenario in unit_tests:
            lines.extend(_generate_test_function(scenario, issue_number))

    # Generate integration tests
    if integration_tests:
        lines.append("")
        lines.append("# Integration Tests")
        lines.append("# -----------------")
        lines.append("")

        for scenario in integration_tests:
            lines.extend(_generate_test_function(scenario, issue_number, fixture="test_client"))

    # Generate E2E tests
    if e2e_tests:
        lines.append("")
        lines.append("# E2E Tests")
        lines.append("# ---------")
        lines.append("")

        for scenario in e2e_tests:
            lines.extend(_generate_test_function(scenario, issue_number, fixture="test_client"))

    # Generate other tests
    if other_tests:
        lines.append("")
        lines.append("# Other Tests")
        lines.append("# -----------")
        lines.append("")

        for scenario in other_tests:
            lines.extend(_generate_test_function(scenario, issue_number))

    # Format issue_number in docstring
    content = "\n".join(lines)
    content = content.replace("{issue_number}", str(issue_number))

    return content


def _generate_test_function(
    scenario: TestScenario,
    issue_number: int,
    fixture: str | None = None,
) -> list[str]:
    """Generate a single test function.

    Args:
        scenario: Test scenario.
        issue_number: GitHub issue number.
        fixture: Optional fixture to include in function signature.

    Returns:
        Lines of the test function.
    """
    name = scenario.get("name", "test_unnamed")
    # Ensure name starts with test_
    if not name.startswith("test_"):
        name = f"test_{name}"

    # Clean up name to be a valid Python identifier
    name = re.sub(r"[^a-zA-Z0-9_]", "_", name)
    name = re.sub(r"_+", "_", name)  # Remove duplicate underscores

    description = scenario.get("description", "")
    requirement_ref = scenario.get("requirement_ref", "")
    assertions = scenario.get("assertions", [])
    mock_needed = scenario.get("mock_needed", False)
    test_type = scenario.get("test_type", "unit").lower()

    lines = []

    # Add pytest markers for non-unit test types
    # This enables e2e_validation.py to filter with '-m e2e or integration'
    if test_type == "e2e":
        lines.append("@pytest.mark.e2e")
    elif test_type == "integration":
        lines.append("@pytest.mark.integration")

    # Function signature
    if fixture:
        if mock_needed:
            lines.append(f"def {name}({fixture}, mock_external_service):")
        else:
            lines.append(f"def {name}({fixture}):")
    elif mock_needed:
        lines.append(f"def {name}(mock_external_service):")
    else:
        lines.append(f"def {name}():")

    # Docstring
    docstring_lines = [f'    """']
    if description:
        # Wrap description at 70 chars
        wrapped = _wrap_text(description, 70)
        for line in wrapped:
            docstring_lines.append(f"    {line}")
    else:
        docstring_lines.append(f"    Test: {name}")

    if requirement_ref:
        docstring_lines.append(f"")
        docstring_lines.append(f"    Requirement: {requirement_ref}")

    if assertions:
        docstring_lines.append(f"")
        docstring_lines.append(f"    Assertions:")
        for assertion in assertions[:3]:  # Limit to 3
            docstring_lines.append(f"    - {assertion}")

    docstring_lines.append(f'    """')
    lines.extend(docstring_lines)

    # Test body with TDD assertion
    lines.append("    # TDD: Arrange")
    lines.append("    # TODO: Set up test data and mocks")
    lines.append("")
    lines.append("    # TDD: Act")
    lines.append("    # TODO: Call the function/method under test")
    lines.append("")
    lines.append("    # TDD: Assert")

    # Add specific assertions as comments if available
    if assertions:
        for assertion in assertions[:3]:
            lines.append(f"    # TODO: {assertion}")
        lines.append("")

    # The TDD assertion that must fail initially
    lines.append(f'    assert False, "TDD: Implementation pending for {name}"')
    lines.append("")
    lines.append("")

    return lines


def _wrap_text(text: str, width: int) -> list[str]:
    """Wrap text at specified width."""
    words = text.split()
    lines = []
    current_line = []
    current_length = 0

    for word in words:
        if current_length + len(word) + 1 <= width:
            current_line.append(word)
            current_length += len(word) + 1
        else:
            if current_line:
                lines.append(" ".join(current_line))
            current_line = [word]
            current_length = len(word)

    if current_line:
        lines.append(" ".join(current_line))

    return lines if lines else [""]


def determine_test_file_path(
    issue_number: int,
    scenarios: list[TestScenario],
    repo_root: Path,
) -> Path:
    """Determine the appropriate path for the test file.

    Args:
        issue_number: GitHub issue number.
        scenarios: List of test scenarios.
        repo_root: Repository root path.

    Returns:
        Path for the test file.
    """
    # Default test directory
    tests_dir = repo_root / "tests"
    tests_dir.mkdir(parents=True, exist_ok=True)

    # Extract module name from scenarios if possible
    # For now, use issue number as identifier
    return tests_dir / f"test_issue_{issue_number}.py"


def scaffold_tests(state: TestingWorkflowState) -> dict[str, Any]:
    """N2: Generate executable test stubs.

    Args:
        state: Current workflow state.

    Returns:
        State updates with test file paths.
    """
    print("\n[N2] Scaffolding tests...")

    # Check for mock mode
    if state.get("mock_mode"):
        return _mock_scaffold_tests(state)

    # Get data from state
    issue_number = state.get("issue_number", 0)
    test_scenarios = state.get("test_scenarios", [])
    repo_root_str = state.get("repo_root", "")
    repo_root = Path(repo_root_str) if repo_root_str else get_repo_root()

    # --------------------------------------------------------------------------
    # GUARD: Validate scenarios
    # --------------------------------------------------------------------------
    if not test_scenarios:
        print("    [GUARD] BLOCKED: No test scenarios to scaffold")
        return {
            "error_message": "GUARD: No test scenarios available",
        }
    # --------------------------------------------------------------------------

    print(f"    Scaffolding {len(test_scenarios)} test scenarios")

    # Determine test file path
    test_file_path = determine_test_file_path(issue_number, test_scenarios, repo_root)
    print(f"    Test file: {test_file_path}")

    # Generate test file content
    module_name = f"issue_{issue_number}"
    content = generate_test_file_content(test_scenarios, module_name, issue_number)

    # Write test file
    test_file_path.write_text(content, encoding="utf-8")
    print(f"    Generated {len(test_scenarios)} tests")

    # Save to audit trail
    audit_dir = Path(state.get("audit_dir", ""))
    if audit_dir.exists():
        file_num = next_file_number(audit_dir)
        save_audit_file(audit_dir, file_num, "test-scaffold.py", content)
    else:
        file_num = state.get("file_counter", 0)

    # Log scaffolding
    log_workflow_execution(
        target_repo=repo_root,
        issue_number=issue_number,
        workflow_type="testing",
        event="tests_scaffolded",
        details={
            "test_file": str(test_file_path),
            "test_count": len(test_scenarios),
        },
    )

    return {
        "test_files": [str(test_file_path)],
        "file_counter": file_num,
        "error_message": "",
    }


def _mock_scaffold_tests(state: TestingWorkflowState) -> dict[str, Any]:
    """Mock implementation for testing."""
    issue_number = state.get("issue_number", 42)
    test_scenarios = state.get("test_scenarios", [])
    repo_root_str = state.get("repo_root", "")
    repo_root = Path(repo_root_str) if repo_root_str else get_repo_root()

    # Generate actual test file for mock mode too
    test_file_path = determine_test_file_path(issue_number, test_scenarios, repo_root)

    if test_scenarios:
        content = generate_test_file_content(test_scenarios, f"issue_{issue_number}", issue_number)
    else:
        content = '''"""Mock test file for testing."""

import pytest


def test_mock_example():
    """Mock test that will fail."""
    assert False, "TDD: Implementation pending for test_mock_example"
'''

    test_file_path.write_text(content, encoding="utf-8")

    # Save to audit
    audit_dir = Path(state.get("audit_dir", ""))
    if audit_dir.exists():
        file_num = next_file_number(audit_dir)
        save_audit_file(audit_dir, file_num, "test-scaffold.py", content)
    else:
        file_num = state.get("file_counter", 0)

    print(f"    [MOCK] Scaffolded tests to {test_file_path}")

    return {
        "test_files": [str(test_file_path)],
        "file_counter": file_num,
        "error_message": "",
    }
